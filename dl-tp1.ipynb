{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3352c55-74f5-4627-bffe-2a62646b9ca8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f56ae059e60c644d20fcf6566e83bde",
     "grade": false,
     "grade_id": "cell-e7f749f1d24233da",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "# IS319 - Deep Learning\n",
    "\n",
    "## TP1 - Neural networks\n",
    "\n",
    "The goal of this TP is to implement a simple feedforward neural network, but without the use of libraries like PyTorch or TensorFlow. We will only use NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a45a5ac-e124-4dd4-8c70-162cf1f303f7",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ce74b31c12c9ae5fc28d9f0bde3045ae",
     "grade": false,
     "grade_id": "cell-00063860a6102415",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b494d22-a981-4cef-9a2f-180e2c03463c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "24c151e63001e49f2b7a0f980115a09b",
     "grade": false,
     "grade_id": "cell-6677b191d92dc6b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## 1. Activation function and its derivative\n",
    "\n",
    "**(Question)** Implement the following activation function and its respective gradient (vector of partial derivatives). These should be applied element-wise to the input vector `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "977beeb6-6308-4a62-a6ec-cd5d7ef4f0af",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3988b2b90122a3e662f7b4f6046337ba",
     "grade": false,
     "grade_id": "activation-functions",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(a):\n",
    "    '''Return the element-wise sigmoid of the input vector.'''\n",
    "    return 1/(1+np.exp(-a))\n",
    "\n",
    "\n",
    "def d_sigmoid(a):\n",
    "    '''Return the partial derivatives of the sigmoid function\n",
    "    with respect to the input vector.'''\n",
    "    # YOUR CODE HER\n",
    "    return (1-sigmoid(a))*sigmoid(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d135483f-3b82-44f3-b7be-469e71de5e42",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b80432cdeab94236cb3e8a7bab77a818",
     "grade": true,
     "grade_id": "activation-functions-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.random.randn(100)\n",
    "assert np.all(sigmoid(a) >= 0.)\n",
    "assert np.all(sigmoid(a) <= 1.)\n",
    "assert sigmoid(0.) == 0.5\n",
    "assert np.all(d_sigmoid(a) >= 0.)\n",
    "assert np.all(d_sigmoid(a) <= 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2496f14-4f65-45aa-81de-7ed6776512f3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf310d3ade385374a6b17ba58a56a68f",
     "grade": false,
     "grade_id": "cell-b5b0c0c62db57fb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Loss function and its derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43e836c-55d0-47c7-a66a-9eadd6e808e2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "338407dbc743f6cc63b292b769db9dc2",
     "grade": false,
     "grade_id": "cell-cf207a1a25ede0bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**(Question)** Implement the following loss function and its respective gradient (vector of partial derivatives).\n",
    "\n",
    "`y` and `d` correspond to predictions and ground-truth labels respectively. They are assumed to be be matrices of size `n_classes * n_samples`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f765ff51-e5f0-4639-bf94-b73ea1a4d729",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3f2df864b322a5c84e3f480a4949ea3d",
     "grade": false,
     "grade_id": "loss-function",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def squared_error(y, d):\n",
    "    '''Return a scalar corresponding to the sum of squared errors.'''\n",
    "    # The sum instead of mean will be more convenient for this TP\n",
    "    # YOUR CODE HERE\n",
    "    delta = y - d\n",
    "    delta = np.power(delta, 2)\n",
    "    loss = np.sum(delta)\n",
    "    return loss\n",
    "\n",
    "def d_squared_error(y, d):\n",
    "    '''Return the vector of partial derivatives of the sum of\n",
    "    squared errors with respect to the predictions.'''\n",
    "    # YOUR CODE HERE\n",
    "    return 2*(y-d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "83c7a3fc-c76f-4d24-afca-25c409a8aff9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5de2abb69d10d0fd30354508f1380637",
     "grade": true,
     "grade_id": "loss-function-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = np.random.randn(3, 100)\n",
    "d = np.random.randn(3, 100)\n",
    "assert squared_error(y, d) >= 0.\n",
    "assert d_squared_error(y, d).shape == y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4dfe238-cce1-447f-bc74-c92e2faff4b3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "67d580cd8275b54d13f713f509624466",
     "grade": false,
     "grade_id": "cell-f5669aa56560f23f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## 3. Neural network architecture\n",
    "\n",
    "We will implement a simple fully-connected neural network with **one hidden layer** and **one output layer**.\n",
    "\n",
    "This neural network is defined by a number of inputs, a number of hidden units, and a number of output units.\n",
    "\n",
    "The activation function will be sigmoid and the loss function will be the sum of squared errors, both implemented above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8bfafb-4db0-4dfa-bf99-232ae3e07d4f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "63330aca7274f43e4e3394272547f540",
     "grade": false,
     "grade_id": "cell-0324c33b4dfea6b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Question)** Complete the class below to initialize the weights and biases randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34b35e8b-ca06-4e03-9121-5d012b89b40d",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c2f866fee5e620ed936080942e9395a",
     "grade": false,
     "grade_id": "init-weights",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        '''Initialize a neural network with `n_input` input neurons,\n",
    "        `n_hidden` hidden neurons and `n_output` output neurons.'''\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        '''Initialize random weights with correct sizes in attributes `W1`, `b1`, `W2` and `b2`.'''\n",
    "        # YOUR CODE HERE\n",
    "        self.W1 = np.random.random((self.n_hidden, self.n_input))\n",
    "        self.b1 = np.random.random((self.n_hidden,1))\n",
    "\n",
    "        self.W2 = np.random.random((self.n_output, self.n_hidden))\n",
    "        self.b2 = np.random.random((self.n_output,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "98ee2249-0ffe-45b9-84aa-43df08dc2ffe",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7d2115947b0270481e8245bc4645ff50",
     "grade": true,
     "grade_id": "init-weights-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(64, 32, 3)\n",
    "assert nn.W1.ndim == 2\n",
    "assert nn.b1.ndim == 2\n",
    "assert nn.W2.ndim == 2\n",
    "assert nn.b2.ndim == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dd53bb-9867-436a-ba06-f12ab5c6ecb9",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fd4391516076bbeb51ee9757c5a9075",
     "grade": false,
     "grade_id": "cell-b38df17eaae875f5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## 4. Forward pass\n",
    "\n",
    "The forward pass is defined as:\n",
    "$$\\begin{align*}\n",
    "\\mathbf{h}_1 &= \\sigma(\\mathbf{a}_1) \\quad\\text{with}\\quad \\mathbf{a}_1 = \\mathbf{W}_1 \\mathbf{x} + \\mathbf{b}_1 \\\\\n",
    "\\mathbf{y} &= \\sigma(\\mathbf{a}_2) \\quad\\text{with}\\quad \\mathbf{a}_2 = \\mathbf{W}_2 \\mathbf{h}_1 + \\mathbf{b}_2\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e029d-6502-428a-997f-023f05b61e17",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "46951d84d019a91fb33375289a4cfc50",
     "grade": false,
     "grade_id": "cell-6c74b62788369be3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Question)** Implement the forward pass for input examples `X`. Save intermediate results `a1`, `h1` and `a2` into attributes (as they will be needed for the backpropagation algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0a17e7b8-f65d-46c5-8dbf-beaa9ae108e1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5184d383ebc7f26abfd5b14931a7a9d9",
     "grade": false,
     "grade_id": "cell-ec6cc8adc2e96480",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork): # (the method will be added to the `NeuralNetwork` class)\n",
    "    def forward(self, X):\n",
    "        # YOUR CODE HERE\n",
    "        self.a1 = (self.W1 @ X) + self.b1 #n_hidden,n\n",
    "        self.h1 = sigmoid(self.a1) \n",
    "        self.a2 = (self.W2 @ self.h1) + self.b2 #n_output,n\n",
    "        \n",
    "        y = sigmoid(self.a2) #(n_output,n)\n",
    "\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4ebfecd-ccf7-4d54-a817-7a8418733511",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d29e31eba2ebf4babaa79ce8d7926b5a",
     "grade": true,
     "grade_id": "forward-pass",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(64, 32, 3)\n",
    "X = np.random.randn(64, 100)\n",
    "y = nn.forward(X)\n",
    "assert y.shape == (3, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0776cbe4-c4dc-4022-bbb6-7a72ea9cd33f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c461ed61d7c941f5dce94612747ec329",
     "grade": false,
     "grade_id": "cell-a74aac18b5d769d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**(Question)** Implement the function below to obtain a classification decision from the network. To do that, apply the forward pass, then choose the class corresponding to the maximum output value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "51c63f4d-c588-4f53-a491-04b771a748ec",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b2e33de7026f7fe84828861f386dd8f",
     "grade": false,
     "grade_id": "predic",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork): # (the method will be added to the `NeuralNetwork` class)\n",
    "    def predict(self, X):\n",
    "        # YOUR CODE HERE\n",
    "        d = self.forward(X)\n",
    "        y = np.array([np.where(d[:,i] == np.max(d[:,i]))[0][0] for i in range(len(X[0]))])\n",
    "        return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24649afc-05ba-458d-b221-65faf4708579",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83077ae274e07b798338dc6f984905a4",
     "grade": true,
     "grade_id": "predict-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(64, 32, 3)\n",
    "X = np.random.randn(64, 100)\n",
    "y = nn.predict(X)\n",
    "assert y.shape == (100,)\n",
    "assert np.any(y == 0) or np.any(y == 1) or np.any(y == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da06ec3-f507-4fff-8083-fee28c07f227",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "111821d4eb26f9ab464706dba8926c59",
     "grade": false,
     "grade_id": "cell-716664c03ec25271",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## 5. Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810de3e4-4745-429d-b413-1fcd01ab78b4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0a53e5a52c8894f784dce2e57159d13",
     "grade": false,
     "grade_id": "cell-a379bc9c9644efe2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**(Question)** Implement the backward pass for input examples `X`, ground-truth `d`, predictions `y`.\n",
    "\n",
    "*Advice 1:* start by working on weights `d_W2` and `d_W1`, then work on the biases `d_b2` and `d_b1`.\n",
    "\n",
    "*Advice 2:* keep track of the shapes of each partial derivatives using comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b0baa1a-0693-42d6-acb6-af801ddd3b38",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6db3f094eb2165efd7873c6c6e4a33cc",
     "grade": false,
     "grade_id": "backward",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def backward(self, X, y, d):\n",
    "        '''Compute the partial derivatives of the loss function\n",
    "        with respect to all weights of the neural network.\n",
    "        Return these in variables `d_W1`, `d_b1`, `d_W2` and `d_b2`.'''\n",
    "        # Backpropagation for the output layer\n",
    "        # You should compute d_ey, d_ya2, d_a2w2 and finally delta2\n",
    "        # Then, you can compute d_W2 and d_b2\n",
    "        d_ey = y - d #(n_output,n)\n",
    "        d_ya2 = d_sigmoid(self.a2) #(n_output,n)\n",
    "        d_a2w2 = self.h1 #(n_hidden,n)\n",
    "        delta2 = d_ey * d_ya2 #(n_output, n)\n",
    "        # YOUR CODE HERE\n",
    "        d_W2 = delta2 @ d_a2w2.T #(n_output, n_hidden)\n",
    "        d_b2 = np.sum(delta2,axis=1).reshape(-1,1) #(n_ouput, 1)\n",
    "\n",
    "        \n",
    "        # Backpropagation for the hidden layer\n",
    "        # You should compute d_h1a1 and finally delta1\n",
    "        # Then, you can compute d_W1 and d_b1\n",
    "        # YOUR CODE HERE\n",
    "        dh1a1 = d_sigmoid(self.a1) #(n_hidden,n)\n",
    "        delta1 = (delta2.T @ self.W2).T * dh1a1 # (n_hidden, n)\n",
    "        d_W1 = delta1 @ X.T  #(n_hidden,n)\n",
    "        d_b1 = np.sum(delta1,axis=1).reshape(-1,1) #(n_hidden,1)\n",
    "\n",
    "        return d_W1, d_b1, d_W2, d_b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d48b45f8-e1ad-4995-b0f7-ebf2f4a5af1f",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b80ad348305b39d3b6cc79bc53b0df33",
     "grade": true,
     "grade_id": "backward-tests",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(64, 32, 3)\n",
    "X = np.random.randn(64, 100)\n",
    "d = np.random.randint(0, 2, size=(3, 100))\n",
    "y = nn.forward(X)\n",
    "loss = squared_error(y, d)\n",
    "d_W1, d_b1, d_W2, d_b2 = nn.backward(X, y, d)\n",
    "assert d_W1.shape == nn.W1.shape\n",
    "assert d_b1.shape == nn.b1.shape\n",
    "assert d_W2.shape == nn.W2.shape\n",
    "assert d_b2.shape == nn.b2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f24a0eb-ef0d-4de5-991e-07d6bf058af5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b31330285a0cc91e3ad7ec91dca3d78",
     "grade": false,
     "grade_id": "cell-0f7f2ccc7c2703a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## 6. Weights update with gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbb7b81-bc3e-402c-87b5-b41a59364635",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "205541f1ac459a92fd5e3bf5a29541ec",
     "grade": false,
     "grade_id": "cell-855bcf3ab4e1f779",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Question)** Complete the following code to implement one iteration of the training process:\n",
    "- Apply the forward pass on training data and compute the loss\n",
    "- Apply backpropagation to compute the gradient of the loss with respect to the network parameters\n",
    "- Apply gradient descent to update the network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "afbfeefb-6232-4b1a-9662-2db27ee41321",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6009615e77ace62600bc917dbc2d306e",
     "grade": false,
     "grade_id": "train-iteration",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def train_iteration(self, X, d, lr=1e-2):\n",
    "        # Apply the forward pass and compute the loss\n",
    "        # YOUR CODE HERE\n",
    "        y_hat = self.forward(X)\n",
    "        d_W1, d_b1, d_W2, d_b2 = self.backward(X,y_hat,d)\n",
    "        self.W1 -= lr*d_W1\n",
    "        self.W2 -= lr*d_W2\n",
    "        self.b1 -= lr*d_b1\n",
    "        self.b2 -= lr*d_b2\n",
    "\n",
    "        loss = squared_error(d,y_hat)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9e69d878-5a2e-457d-b0ad-76f2915909a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de24a0544915c44c8d2f86935b79e7a6",
     "grade": true,
     "grade_id": "train-iteration-tests",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(64, 32, 3)\n",
    "X = np.random.randn(64, 100)\n",
    "d = np.random.randint(0, 2, size=(3, 100))\n",
    "loss = nn.train_iteration(X, d, lr=100)\n",
    "assert loss >= 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375846fe-7894-4467-9245-ce02b845d587",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07572dc8db8e5ec03b5e7dd135cbe8f8",
     "grade": false,
     "grade_id": "cell-f6f5fa4a0e6feedc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## 7. Mini-batch training loop\n",
    "\n",
    "Now, we will implement the main training loop of our neural network.\n",
    "\n",
    "We will use stochastic gradient descent with mini-batch: the weights will be updated by performing gradient descent on shuffled subsets of training data.\n",
    "\n",
    "We will train the network for a number of epochs (an epoch is performed when the whole training set has been used with this mini-batch procedure)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca926833-fb23-4acf-925a-fdb6fa6ba4c6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "969c109816b302f070575f1c39afe96a",
     "grade": false,
     "grade_id": "cell-42184f408b95fa4c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Question)** Complete the code below to implement the training loop with minibatch stochastic gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "130674e9-0101-4192-baf0-312abdbd0134",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e957b7273b5f45ad30f182a0d35cc1a",
     "grade": true,
     "grade_id": "training-loop",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork(NeuralNetwork):\n",
    "    def fit(self, X, d, batch_size, n_epochs=10, lr=1e-2):\n",
    "        n_samples = X.shape[1]\n",
    "        n_batches = (n_samples // batch_size) + 1\n",
    "        epoch_loss = 0.\n",
    "        for e in range(n_epochs):\n",
    "            losses = []\n",
    "            # Shuffle dataset\n",
    "            permutation = np.random.permutation(n_samples)\n",
    "            X, d = X[:, permutation], d[:, permutation]\n",
    "            # Loop over each batch\n",
    "            for b in range(0, n_samples, batch_size): # range(start, stop, step)\n",
    "                # Grab the current batch in `X_batch` and `d_batch`\n",
    "                # YOUR CODE HERE\n",
    "                X_batch, y_batch = X[b:b+batch_size], y[b:batch_size]\n",
    "                # Apply training iteration and update epoch loss\n",
    "                # YOUR CODE HERE\n",
    "                losses = self.train_iteration(X,d,lr)\n",
    "                epoch_loss += loss\n",
    "                #losses.append(loss)\n",
    "            # Compute average epoch loss and print it\n",
    "            #print(f'epoch {e} avg loss : {sum(losses)/len(losses)}')\n",
    "            print(f'epoch {e} cumulative loss : {epoch_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2116cae8-1f48-45a4-b1d7-bf5cd35e97d4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54b57ad4d5d9a87af99087e8fa8f033e",
     "grade": false,
     "grade_id": "cell-c980fa7b2fa057b9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## 8. Train the network on the MNIST dataset\n",
    "\n",
    "The MNIST dataset is composed of 70000 greyscale images of handwritten digits: 60000 images for training and 10000 for testing.\n",
    "\n",
    "It is included in the `mnist.tgz` archive provided with this TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e1e3e19-c599-4633-b815-a5af3afe1cdb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef0c69a8ba2ce881cb101b8a63d2ef5f",
     "grade": false,
     "grade_id": "cell-5562efce16521bb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x mnist-test-images.npy\n",
      "x mnist-test-labels.npy\n",
      "x mnist-train-images.npy\n",
      "x mnist-train-labels.npy\n"
     ]
    }
   ],
   "source": [
    "!tar xvzf ./mnist.tgz\n",
    "images_train = np.load('./mnist-train-images.npy')\n",
    "labels_train = np.load('./mnist-train-labels.npy')\n",
    "images_test = np.load('./mnist-test-images.npy')\n",
    "labels_test = np.load('./mnist-test-labels.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60674f9f-1b2d-481c-8d9f-6427ea608d93",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4acf046d404e63e8f9cf998a58328f22",
     "grade": false,
     "grade_id": "cell-c3a9fec3fa080314",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**(Question)** Reshape the images into vectors and normalize the pixel values between 0 and 1. Convert the labels into one-hot vectors (*i.e.* vectors full of 0 and with only a 1 for the corresponding class). Store the results into `X_train`, `y_train`, `X_test` and `y_test` variables. Make sure to reshape to the following:\n",
    "- Input data: `n_features x n_samples`\n",
    "- Labels: `n_classes x n_samples`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84e60aea-3db1-4bb2-a394-ce7f51ac3ca8",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "23e56a7ce963ee1576ef578d6cb42b90",
     "grade": false,
     "grade_id": "mnist-dataset",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,) (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "X_train = np.load('mnist-train-images.npy')\n",
    "y_train = np.load('mnist-train-labels.npy')\n",
    "X_test = np.load('mnist-test-images.npy')\n",
    "y_test = np.load('mnist-test-labels.npy')\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8418ceb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000) (784, 10000)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape( X_train.shape[1] * X_train.shape[2],X_train.shape[0])\n",
    "X_test = X_test.reshape(X_test.shape[1] * X_test.shape[2],X_test.shape[0])\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "16048e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "mask = np.isnan(X_test)\n",
    "print(len(mask[mask == True]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "723e89b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 255 255 0\n"
     ]
    }
   ],
   "source": [
    "min_train, max_train = np.min(X_train), np.max(X_train)\n",
    "min_test, max_test = np.min(X_test), np.max(X_test)\n",
    "print(min_train,max_test,max_train,min_test)\n",
    "X_train = (X_train - min_train) / (max_train - min_train)\n",
    "X_test = (X_test - min_test) / (max_train - min_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a8d9ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 9\n"
     ]
    }
   ],
   "source": [
    "print(np.max(y_train), np.max(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e689bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes):\n",
    "    id_mat = np.eye(num_classes)\n",
    "    one_hot_encoded = id_mat[y]\n",
    "    return one_hot_encoded.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fc547693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 60000) (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "y_train, y_test = to_categorical(y_train,10), to_categorical(y_test,10)\n",
    "\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b753e345-4d69-47d3-bc2d-4a05f4c09766",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f68ed5893b3a3657bd36d0fe74efd59d",
     "grade": true,
     "grade_id": "mnist-dataset-tests",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert np.all(X_train >= 0.) and np.all(X_train <= 1.)\n",
    "assert np.all(X_test >= 0.) and np.all(X_test <= 1.)\n",
    "assert np.all(np.unique(y_train) == np.array([0., 1.])) \n",
    "assert np.all(np.unique(y_test) == np.array([0., 1.]))\n",
    "assert np.all(np.sum(y_train, axis=0) == 1.)\n",
    "assert np.all(np.sum(y_test, axis=0) == 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c092ea-0342-4d64-89b9-c8ed5a1a2ad4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd5c0f6c9bd960d9ea0b3cc726d41e90",
     "grade": false,
     "grade_id": "cell-6c0d93b6eb6561df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**(Question)** Initialize a neural network for MNIST with 32 hidden units and train it for 10 epochs with a batch size of 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "77f0664e-95cb-4b4f-a711-ca5d7b067df2",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a283bf0348d1f16794309fcddc49b38f",
     "grade": true,
     "grade_id": "mnist-train",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayman\\AppData\\Local\\Temp\\ipykernel_82660\\3101076523.py:3: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-a))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 cumulative loss : 15582.396452173789\n",
      "epoch 1 cumulative loss : 31164.792904347632\n",
      "epoch 2 cumulative loss : 46747.18935652109\n",
      "epoch 3 cumulative loss : 62329.58580869451\n",
      "epoch 4 cumulative loss : 77911.98226086792\n",
      "epoch 5 cumulative loss : 93494.37871304134\n",
      "epoch 6 cumulative loss : 109076.77516521476\n",
      "epoch 7 cumulative loss : 124659.17161738817\n",
      "epoch 8 cumulative loss : 140241.5680695616\n",
      "epoch 9 cumulative loss : 155823.964521735\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "nn = NeuralNetwork(n_input=X_train.shape[0],n_hidden=32,n_output=y_train.shape[0])\n",
    "nn.fit(X_train, y_train,n_epochs=10, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ceed676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f971d1f-a2be-4e14-8b32-d16cf95133bb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0702549a613d49feff7dfd5b4178c833",
     "grade": false,
     "grade_id": "cell-c428e777c7ce4fc2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**(Question)** Compute the classification accuracy on the train and test sets. To do that, you can use the predict function and compare them with the original labels (*i.e.* without one-hot encoding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c0079310-dc37-4e09-a3d8-802d20f5c9ca",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "374097f8ed66b3dc5b906d6fbfcb630e",
     "grade": true,
     "grade_id": "mnist-accuracy",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ayman\\Downloads\\dl-tp1.ipynb Cell 47\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayman/Downloads/dl-tp1.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ayman/Downloads/dl-tp1.ipynb#X54sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m()\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea0b13d-ff16-4a77-b031-d6b770d8d542",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f1460ad42591ceee6c3ad8d6cd56e8f2",
     "grade": false,
     "grade_id": "cell-d57ca20154998a71",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "tags": []
   },
   "source": [
    "**(Question)** Compute and plot the confusion matrix for the test set. Which are the most difficult classes? Show some examples of misclassified images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd429c3b-0caa-4404-abd1-90634a141887",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "55d3c78fffbc672cda2e5e5829a1f99e",
     "grade": true,
     "grade_id": "mnist-results",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e2034c-1258-4e01-945f-6f1b5907dd7c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "68541cac2f30c21ce5fa54188105962c",
     "grade": false,
     "grade_id": "cell-7d1885888535c8ea",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "**(Question)** Play around with hyperparameters of the model. What happens when the batch size if very small? And very large?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd58d50-39f8-401b-b84e-b3cf5c060cf6",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0df07798cf703fcd5235339f0d58aaa8",
     "grade": true,
     "grade_id": "mnist-hyperparameters",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5c9c3-b3fc-424c-b815-83c47a704229",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6d40b357e3b5b8ed50650a2319ba0196",
     "grade": true,
     "grade_id": "mnist-comments",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7102a2-a15a-4998-a1ce-c060d64a4e83",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a5b965e5a491c8593296182c01270c6",
     "grade": false,
     "grade_id": "cell-0bf423527a0279d3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 9. Extension to more than one hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f9f104-9ab8-46f1-97a3-93b714e6b018",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bf0b3fda55676027ae5b8e965c874403",
     "grade": false,
     "grade_id": "extend-multiple-layers",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "tags": []
   },
   "source": [
    "**(Question)** Extend your neural network model to handle more than one hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60a109-f66a-4527-a9ae-891d4525d667",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7483f6ba66ef4dd2a928c91f00a9f5c",
     "grade": true,
     "grade_id": "cell-ab36027b7515bf15",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fccc60a-4f7d-4e31-827f-39b2791c1713",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44dc156b6626b3e62b9a19658c28a9ce",
     "grade": false,
     "grade_id": "cell-60e9495352b554be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "source": [
    "## 10. Extension to softmax and categorical cross-entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d36216-95f5-4db1-b83d-39942e09df34",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c9554f96bc8af873f05114d2fb594b2e",
     "grade": false,
     "grade_id": "extend-crossentropy",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    },
    "tags": []
   },
   "source": [
    "**(Question)** Extend your neural network model to use a softmax activation function for the output layer, and a categorical cross-entropy loss.\n",
    "You can also experiment with the reLU activation for the hidden layer.\n",
    "\n",
    "*Hint:* recall the partial derivatives formulation from logistic regression, and optimize the backpropagation for the output layer accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c24019-4f36-497f-ad3e-a42ce1a8ad79",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4f3434f56af08fae4b878f1f4e02f7f3",
     "grade": true,
     "grade_id": "cell-30584782a093acaf",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f10e5f-d858-45aa-978b-723fefd68bcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba97f9f0-c2b6-4c0b-b3e4-56d9a05be1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.randn(100,64).T\n",
    "y = np.random.randint(0, 10, size=100)\n",
    "\n",
    "y_onehot = np.eye(10)[y].T\n",
    "\n",
    "\n",
    "model = NeuralNetwork(64, 32, 10) #initializes already W,b's randomly\n",
    "\n",
    "alpha = 1.\n",
    "model.W1 *= alpha\n",
    "model.b1 *= alpha\n",
    "..\n",
    "y_pred = nn.softmax(model.forward(X))\n",
    "loss = nn.categorical.crossentropy(y_pred, y_onehot)\n",
    "\n",
    "#this makes the output as a probability distribution \n",
    "#usually to judge if our model is good or not, we compare the loss of the crossentropy with -log(1/K) with K the number of classes\n",
    "#-log(1/K) is the crossentropy loss that a random mean classifier would gives us (probability 1/10 for each class : look the formula and replace)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
