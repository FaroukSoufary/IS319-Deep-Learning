{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abb49b5c-7533-4bf0-bcce-37373d6fa072",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1502bbeee981ecc3a7a4464df92be0c",
     "grade": false,
     "grade_id": "cell-a975d7bfb06f30ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# IS319 - Deep Learning\n",
    "\n",
    "## TP3 - Recurrent neural networks\n",
    "\n",
    "Credits: Andrej Karpathy\n",
    "\n",
    "The goal of this TP is to experiment with recurrent neural networks for a character-level language model to generate text that looks like training text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "16d03ccd-089a-4553-bd96-7bd0c0f71949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c736b5-43df-4aca-9ba3-dd34ed8366f4",
   "metadata": {},
   "source": [
    "## 1. Text data preprocessing\n",
    "\n",
    "Several text datasets are provided, feel free to experiment with different ones throughout the TP. At the beginning, use a small subset of a given dataset (for example use only 10k characters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e56a81b9-b733-4d87-b8d7-2184d5e5c2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset `lotr.txt` contains 10000 characters.\n",
      "Excerpt of the dataset:\n",
      "Three Rings for the Elven-kings under the sky,\n",
      "               Seven for the Dwarf-lords in their halls of stone,\n",
      "            Nine for Mortal Men doomed to die,\n",
      "              One for the Dark Lord on his dark throne\n",
      "           In the Land of Mordor where the Shadows lie.\n",
      "               One Ring to rule them all, One Ring to find them,\n",
      "               One Ring to bring them all and in the darkness bind them\n",
      "           In the Land of Mordor where the Shadows lie.\n",
      "           \n",
      "FOREWORD\n",
      "\n",
      "This tale grew in the telling, until it became a history of the Great War of the Ring and included many glimpses of the yet more ancient history that preceded it. It was begun soon after _The Hobbit_ was written and before its publication in 1937; but I did not go on with this sequel, for I wished first to complete and set in order the mythology and legends of the Elder Days, which had then been taking shape for some years. I desired to do this for my own satisfaction, and I had little hope that other people would be interested in this work, especially since it was primarily linguistic in inspiration and was begun in order to provide the necessary background of 'history' for Elvish tongues.\n",
      "     When those whose advice and opinion I sought corrected _little hope_ to _no hope,_ I went back to the sequel, encouraged by requests from readers for more information concerning hobbits and their adventures. But the story was drawn irresistibly towards the older world, and became an account, as it were, of its end and passing away before its beginning and middle had been told. The process had begun in the writing of _The Hobbit,_ in which there were already some references to the older matter: Elrond, Gondolin, the High-elves, and the orcs, as well as glimpses that had arisen unbidden of things higher or deeper or darker than its surface: Durin, Moria, Gandalf, the Necromancer, the Ring. The discovery of the significance of these glimpses and of their relation to the ancient histories revealed the \n"
     ]
    }
   ],
   "source": [
    "# text_data_fname = 'baudelaire.txt'  # ~0.1m characters (French)\n",
    "# text_data_fname = 'proust.txt'      # ~7.3m characters (French)\n",
    "# text_data_fname = 'shakespeare.txt' # ~0.1m characters (English)\n",
    "text_data_fname = 'lotr.txt'        # ~2.5m characters (English)\n",
    "# text_data_fname = 'doom.c'          # ~1m characters (C Code)\n",
    "# text_data_fname = 'linux.c'         # ~11.5m characters (C code)\n",
    "\n",
    "text_data = open(text_data_fname, 'r').read()\n",
    "text_data = text_data[:10000] # use a small subset\n",
    "print(f'Dataset `{text_data_fname}` contains {len(text_data)} characters.')\n",
    "print('Excerpt of the dataset:')\n",
    "print(text_data[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b9a404-8b09-4ab1-9529-98ff57650ef5",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d6b030743489f86ece30d79835434297",
     "grade": false,
     "grade_id": "cell-9695c6f2cf95337c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Question)** Create a character-level vocabulary for your text data. Create two dictionaries: `ctoi` mapping each character to an index, and the reverse `itoc` mapping each index to its corresponding character. Implement the functions to convert text to tensor and tensor to text using these mappings. Apply these functions to some text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "885ecc8a-c654-463b-9353-f7a18a607199",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3eb46710b4006993f4eb9c557a2376f",
     "grade": true,
     "grade_id": "vocabulary",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'H', '-', ',', ';', ':', '8', 'l', 'E', 'Y', 'u', '(', 'o', 'L', 'c', 's', 'y', '6', ')', 'S', '_', 'G', '4', '.', 'C', '7', 'f', 'P', 'n', 'r', 'I', 'w', 'm', 'B', 'ó', 'v', 'D', '\"', '3', 'e', 'R', 'b', 'é', ' ', 'W', 'g', 'd', 'k', 'F', 't', 'p', \"'\", 'A', 'T', 'q', 'x', 'û', 'h', 'M', 'z', 'O', '9', '1', 'N', 'a', '\\n', 'j']\n",
      "{0: 'i', 1: 'H', 2: '-', 3: ',', 4: ';', 5: ':', 6: '8', 7: 'l', 8: 'E', 9: 'Y', 10: 'u', 11: '(', 12: 'o', 13: 'L', 14: 'c', 15: 's', 16: 'y', 17: '6', 18: ')', 19: 'S', 20: '_', 21: 'G', 22: '4', 23: '.', 24: 'C', 25: '7', 26: 'f', 27: 'P', 28: 'n', 29: 'r', 30: 'I', 31: 'w', 32: 'm', 33: 'B', 34: 'ó', 35: 'v', 36: 'D', 37: '\"', 38: '3', 39: 'e', 40: 'R', 41: 'b', 42: 'é', 43: ' ', 44: 'W', 45: 'g', 46: 'd', 47: 'k', 48: 'F', 49: 't', 50: 'p', 51: \"'\", 52: 'A', 53: 'T', 54: 'q', 55: 'x', 56: 'û', 57: 'h', 58: 'M', 59: 'z', 60: 'O', 61: '9', 62: '1', 63: 'N', 64: 'a', 65: '\\n', 66: 'j'}\n",
      "{'i': 0, 'H': 1, '-': 2, ',': 3, ';': 4, ':': 5, '8': 6, 'l': 7, 'E': 8, 'Y': 9, 'u': 10, '(': 11, 'o': 12, 'L': 13, 'c': 14, 's': 15, 'y': 16, '6': 17, ')': 18, 'S': 19, '_': 20, 'G': 21, '4': 22, '.': 23, 'C': 24, '7': 25, 'f': 26, 'P': 27, 'n': 28, 'r': 29, 'I': 30, 'w': 31, 'm': 32, 'B': 33, 'ó': 34, 'v': 35, 'D': 36, '\"': 37, '3': 38, 'e': 39, 'R': 40, 'b': 41, 'é': 42, ' ': 43, 'W': 44, 'g': 45, 'd': 46, 'k': 47, 'F': 48, 't': 49, 'p': 50, \"'\": 51, 'A': 52, 'T': 53, 'q': 54, 'x': 55, 'û': 56, 'h': 57, 'M': 58, 'z': 59, 'O': 60, '9': 61, '1': 62, 'N': 63, 'a': 64, '\\n': 65, 'j': 66}\n",
      "Sample : Three Ring\n",
      "Tensor of sample : tensor([53, 57, 29, 39, 39, 43, 40,  0, 28, 45], dtype=torch.int32)\n",
      "Text of tensor : Three Ring\n"
     ]
    }
   ],
   "source": [
    "# Create the vocabulary and the two mapping dictionaries\n",
    "# YOUR CODE HERE\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "vocabulary = list(set(text_data))\n",
    "\n",
    "print(vocabulary)\n",
    "\n",
    "ctoi = {vocabulary[num]: num for num in range(0, len(vocabulary))}\n",
    "\n",
    "itoc = {num : vocabulary[num] for num in range(0, len(vocabulary))}\n",
    "\n",
    "print(itoc)\n",
    "print(ctoi)\n",
    "\n",
    "# Implement the function converting text to tensor\n",
    "def text_to_tensor(text, ctoi):\n",
    "    # YOUR CODE HERE\n",
    "    tensor = np.array([ctoi[c] for c in text])\n",
    "    tensor = torch.Tensor(tensor).int()\n",
    "\n",
    "    return tensor \n",
    "\n",
    "# Implement the function converting tensor to text\n",
    "def tensor_to_text(tensor, itoc):\n",
    "    # YOUR CODE HERE\n",
    "    tensor = tensor.numpy()\n",
    "    text = [itoc[i] for i in tensor]\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply your functions to some text data\n",
    "# YOUR CODE HERE\n",
    "sample = text_data[:10]\n",
    "\n",
    "print(\"Sample : \" + str(sample))\n",
    "\n",
    "s_tensor = text_to_tensor(sample, ctoi)\n",
    "s_text = tensor_to_text(s_tensor, itoc)\n",
    "\n",
    "print(\"Tensor of sample : \" + str(s_tensor))\n",
    "print(\"Text of tensor : \" + \"\".join(s_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da1b7f-549d-45d9-8fea-309ee0e76a90",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbfd5a4df6e6081581ad38d7180fddfb",
     "grade": false,
     "grade_id": "cell-172b8befc4633227",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Setup a character-level recurrent neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d33cfac-93f7-4980-8510-fcf675d3a06b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6cb30929efdd10eb6066750f4b94bf14",
     "grade": false,
     "grade_id": "embedding",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Setup a simple embedding layer with `nn.Embedding` to project character indices to `embedding_dim` dimensional vectors. Explain precisely how this layer works and what are its outputs for a given input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "609747a0-6634-4232-92cb-72946ef516b0",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a9dd84493589d73c13fac37411610a2e",
     "grade": true,
     "grade_id": "cell-a3b5adeb8111779b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "embedding_dim = 10 # ?\n",
    "\n",
    "embedding_layer = nn.Embedding(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cd338f5d-41e9-4c6f-8510-be5ad74e1283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2222, -0.2497,  0.5205, -1.1818, -0.3512,  0.0829,  0.0615, -0.9148,\n",
      "         -0.8751, -0.4869],\n",
      "        [-2.4835, -0.4966,  0.9651,  0.3900,  2.0285, -0.4262,  0.3173,  0.6685,\n",
      "          0.1133, -1.0985],\n",
      "        [ 0.6244, -0.9654,  0.6433, -0.2205, -0.0602, -1.1424, -0.2153, -1.4845,\n",
      "         -2.9274, -1.4334],\n",
      "        [-0.9708,  0.8360, -0.6122, -1.8138,  0.7967,  1.0027,  0.3080,  2.0241,\n",
      "         -1.6984, -0.6595],\n",
      "        [-0.9708,  0.8360, -0.6122, -1.8138,  0.7967,  1.0027,  0.3080,  2.0241,\n",
      "         -1.6984, -0.6595],\n",
      "        [-0.3834, -0.4818,  0.8683,  1.1925, -0.0331,  0.1693,  0.9464,  0.8296,\n",
      "         -0.7374, -0.6310],\n",
      "        [-0.6513,  0.4812,  0.5314,  0.1909,  0.2617,  0.7559, -2.2369,  0.4191,\n",
      "          0.3797,  0.4100],\n",
      "        [-0.1366, -0.8664, -0.1562, -0.1915,  0.8270,  0.0253, -0.5566, -0.6141,\n",
      "          0.1634,  1.1352],\n",
      "        [ 0.6390,  0.9776,  0.2482, -0.6635,  1.8979,  0.6915, -1.0867, -1.8876,\n",
      "         -1.3236, -0.7577],\n",
      "        [ 0.0688, -0.8717,  0.3153, -0.7017, -0.4753,  0.3790, -0.8753, -0.3485,\n",
      "         -0.3795,  1.2855]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Test \n",
    "\n",
    "sample = text_data[:10]\n",
    "\n",
    "sample_tensor = text_to_tensor(sample, ctoi)\n",
    "\n",
    "s_embedding = embedding_layer(sample_tensor)\n",
    "\n",
    "print(s_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7a5e3-f137-49ef-a135-a855029b724b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ba287efa3849a2ddcb98b94b287fd199",
     "grade": true,
     "grade_id": "cell-4065672821a5801f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebc7b1c-6469-4c63-9965-7a7f39734302",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6897eae892e4aa50cceac9bca0a83f82",
     "grade": false,
     "grade_id": "base-rnn",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Setup a single-layer RNN with `nn.RNN` (without defining a custom class). Use `hidden_dim` size for hidden states. Explain precisely the outputs of this layer for a given input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "162dd7af-cf96-4279-b512-1507433c7826",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "095b1c904c7c5fa3d5371de208f7300a",
     "grade": true,
     "grade_id": "cell-c0b3cdd14603b6d1",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "hidden_dim = 10\n",
    "\n",
    "simple_rnn = nn.Sequential(\n",
    "    embedding_layer,  \n",
    "    nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, batch_first=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "818b8e0d-bd23-4ef0-b992-4035e140fe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4989,  0.4847, -0.2031,  0.3047, -0.7470,  0.3420,  0.0329, -0.3772,\n",
      "          0.0595,  0.2205],\n",
      "        [ 0.6564,  0.7275,  0.5441,  0.0686, -0.7717,  0.6560,  0.3799,  0.1070,\n",
      "         -0.8785,  0.1572],\n",
      "        [ 0.7917,  0.8228, -0.8421,  0.8381, -0.9041,  0.5942,  0.0492,  0.1503,\n",
      "         -0.0644, -0.5926],\n",
      "        [-0.4491,  0.6646,  0.3512,  0.2249, -0.4473,  0.1599, -0.3692, -0.1253,\n",
      "         -0.4995,  0.6542],\n",
      "        [-0.4573,  0.6285,  0.2699, -0.4040, -0.5353, -0.2194, -0.2508, -0.4276,\n",
      "         -0.2011,  0.5817],\n",
      "        [ 0.2645,  0.6063, -0.0918,  0.1322,  0.1753,  0.5787,  0.1951,  0.5038,\n",
      "         -0.4850, -0.1601],\n",
      "        [-0.7393,  0.5371,  0.5715, -0.4982,  0.0740,  0.3937, -0.4383, -0.4787,\n",
      "         -0.3877,  0.8461],\n",
      "        [-0.0964, -0.4158,  0.4477, -0.5916,  0.0222,  0.2129,  0.1277, -0.3081,\n",
      "          0.0237,  0.0901],\n",
      "        [ 0.7715, -0.1714, -0.8914, -0.0446, -0.9030,  0.2613, -0.4587, -0.8389,\n",
      "          0.1010, -0.2958],\n",
      "        [-0.4957, -0.6075,  0.4166, -0.0456, -0.0434,  0.1913, -0.2061, -0.0687,\n",
      "         -0.1352,  0.5093]], grad_fn=<SqueezeBackward1>)\n",
      "tensor([[-0.4957, -0.6075,  0.4166, -0.0456, -0.0434,  0.1913, -0.2061, -0.0687,\n",
      "         -0.1352,  0.5093]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "sample_output, hid = simple_rnn(text_to_tensor(sample, ctoi))\n",
    "print(sample_output)\n",
    "print(hid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56780e84-8767-4941-ba0a-2047b106fc35",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2f3230528c7fabd5b26931934ac4e8b0",
     "grade": true,
     "grade_id": "cell-9c6c3e3359e2c37e",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76df8c7a-2a3d-43a0-9666-a797d1a3dbca",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3993b8b02d0be0e16de189c4850bbfce",
     "grade": false,
     "grade_id": "rnn-model",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Create a simple RNN model with a custom `nn.Module` class. It should contain: an embedding layer, a single-layer RNN, and a dense output layer. For each character of the input sequence, the model should predict the probability of the next character. The forward method should return the probabilities for next characters and the corresponding hidden states.\n",
    "After completing the class, create a model and apply the forward pass on some input text. Understand and explain the results.\n",
    "\n",
    "*Note:* depending on how you implement the loss function later, it can be convenient to return logits instead of probabilities, i.e. raw values of the output layer before any activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6cc835a8-a05f-4f78-b50e-4edbdb04305f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b587c6f505f5d3719869ae8a57fcb5c6",
     "grade": true,
     "grade_id": "cell-e55d41dd89bcf74f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0130, 0.0140, 0.0072, 0.0204, 0.0099, 0.0212, 0.0087, 0.0095, 0.0294,\n",
      "        0.0156, 0.0198, 0.0117, 0.0140, 0.0127, 0.0198, 0.0129, 0.0155, 0.0258,\n",
      "        0.0205, 0.0132, 0.0048, 0.0107, 0.0134, 0.0185, 0.0210, 0.0235, 0.0076,\n",
      "        0.0141, 0.0128, 0.0058, 0.0232, 0.0167, 0.0189, 0.0094, 0.0164, 0.0152,\n",
      "        0.0107, 0.0224, 0.0169, 0.0103, 0.0214, 0.0098, 0.0236, 0.0134, 0.0236,\n",
      "        0.0157, 0.0246, 0.0090, 0.0109, 0.0061, 0.0095, 0.0115, 0.0125, 0.0103,\n",
      "        0.0138, 0.0072, 0.0308, 0.0209, 0.0138, 0.0098, 0.0150, 0.0199, 0.0177,\n",
      "        0.0096, 0.0188, 0.0066, 0.0070], grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_85482/4257122716.py:37: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probabilities = F.softmax(logits)\n"
     ]
    }
   ],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
    "        '''Initialize model parameters and layers.'''\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim)\n",
    "        self.lin = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "\n",
    "    def forward(self, tensor_data, hidden_state=None):\n",
    "        '''Apply the forward pass for some text data already converted to tensor.'''\n",
    "        # YOUR CODE HERE\n",
    "        out = self.emb(tensor_data)\n",
    "        out, hid = self.rnn(out, hidden_state)\n",
    "        out = self.lin(out)\n",
    "        \n",
    "        return out, hid\n",
    "\n",
    "# Initialize a model and apply the forward pass on some input text\n",
    "# YOUR CODE HERE\n",
    "\n",
    "embedding_dim = 15\n",
    "hidden_dim = 10\n",
    "\n",
    "model = CharRNN(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "sample = \"hello world\"\n",
    "\n",
    "sample_tensor = text_to_tensor(sample, ctoi)\n",
    "\n",
    "logits, hidden = model(sample_tensor)\n",
    "\n",
    "probabilities = F.softmax(logits)\n",
    "\n",
    "print(probabilities[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eb09a2-2c5e-4dd6-922e-a7f0c6cbff85",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ae8f4a875df816da1463fc833e40cea4",
     "grade": true,
     "grade_id": "cell-093600fc493e6e4f",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41614741-8bf2-4ead-8205-788623404a27",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9bb6ca5b49b7c8b46253cf2a7f1200c5",
     "grade": false,
     "grade_id": "rnn-overfit",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Question)** Implement a simple training loop to overfit on a small input sequence. The loss function should be a categorical cross entropy on the predicted characters. Monitor the loss function value over the iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "474f97eb-685f-41c1-8249-1eb4df9e168c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa15210da5aabdca4ecf3228efec88be",
     "grade": true,
     "grade_id": "cell-1904f4989149b1ef",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 3.1742\n",
      "Epoch [20/200], Loss: 2.7236\n",
      "Epoch [30/200], Loss: 2.5126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/200], Loss: 2.3927\n",
      "Epoch [50/200], Loss: 2.3156\n",
      "Epoch [60/200], Loss: 2.2267\n",
      "Epoch [70/200], Loss: 2.1615\n",
      "Epoch [80/200], Loss: 2.1106\n",
      "Epoch [90/200], Loss: 2.0699\n",
      "Epoch [100/200], Loss: 2.0269\n",
      "Epoch [110/200], Loss: 1.9868\n",
      "Epoch [120/200], Loss: 1.9448\n",
      "Epoch [130/200], Loss: 1.8908\n",
      "Epoch [140/200], Loss: 1.8046\n",
      "Epoch [150/200], Loss: 1.7258\n",
      "Epoch [160/200], Loss: 1.6538\n",
      "Epoch [170/200], Loss: 1.5945\n",
      "Epoch [180/200], Loss: 1.5458\n",
      "Epoch [190/200], Loss: 1.4963\n",
      "Epoch [200/200], Loss: 1.4491\n"
     ]
    }
   ],
   "source": [
    "# Sample a small input sequence into tensor `input_seq` and store its corresponding expected sequence into tensor `target_seq`\n",
    "# YOUR CODE HERE\n",
    "\n",
    "# Implement a training loop overfitting an input sequence and monitoring the loss function\n",
    "def train_overfit(model, input_seq, target_seq, n_iters=200, learning_rate=0.2):\n",
    "    # YOUR CODE HERE\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "    for i in range(n_iters):\n",
    "        outputs, _ = model(input_seq)\n",
    "        loss = criterion(outputs, target_seq)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if((i+1)%10==0):\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(i+1, n_iters, loss.item()))\n",
    "\n",
    "    \n",
    "# Initialize a model and make it overfit the input sequence\n",
    "# YOUR CODE HERE\n",
    "\n",
    "def one_hot(input_seq, target_seq, vocab_size, ctoi):\n",
    "    one_hot_vec = np.zeros((len(input_seq), vocab_size))\n",
    "    for i, c in enumerate(input_seq):\n",
    "        t_index = ctoi[target_seq[i]]\n",
    "        one_hot_vec[i][t_index] = 1\n",
    "    return torch.Tensor(one_hot_vec)\n",
    "\n",
    "\n",
    "input_seq = text_data[:100]\n",
    "target_seq = text_data[1:101]\n",
    "\n",
    "target_seq = one_hot(input_seq, target_seq, vocab_size, ctoi)\n",
    "input_seq = text_to_tensor(input_seq, ctoi)\n",
    "\n",
    "embedding_dim = 5\n",
    "hidden_dim = 3\n",
    "\n",
    "model = CharRNN(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "train_overfit(model, input_seq, target_seq, learning_rate=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c66f34-7afb-4920-b6d2-5642d28a0770",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cd6d0a90166cea96ce250f8d8f2e083",
     "grade": false,
     "grade_id": "rnn-argmax",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Implement a `predict_argmax` method for your `RNN` model. Then, verify your overfitting: use some characters of your input sequence as context to predict the remaining ones. Experiment with the current model and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4d0350ae-a23a-4802-a2a5-012193ede15b",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0114ed939e7dd8b656bd0f03bd576a89",
     "grade": true,
     "grade_id": "cell-2d706c010aeccb0d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 2.9891\n",
      "Epoch [20/200], Loss: 2.6086\n",
      "Epoch [30/200], Loss: 2.3733\n",
      "Epoch [40/200], Loss: 2.1996\n",
      "Epoch [50/200], Loss: 2.0738\n",
      "Epoch [60/200], Loss: 1.9795\n",
      "Epoch [70/200], Loss: 1.8997\n",
      "Epoch [80/200], Loss: 1.8296\n",
      "Epoch [90/200], Loss: 1.7662\n",
      "Epoch [100/200], Loss: 1.7081\n",
      "Epoch [110/200], Loss: 1.6550\n",
      "Epoch [120/200], Loss: 1.6058\n",
      "Epoch [130/200], Loss: 1.6138\n",
      "Epoch [140/200], Loss: 1.5629\n",
      "Epoch [150/200], Loss: 1.4916\n",
      "Epoch [160/200], Loss: 1.4449\n",
      "Epoch [170/200], Loss: 1.4073\n",
      "Epoch [180/200], Loss: 1.3615\n",
      "Epoch [190/200], Loss: 1.3545\n",
      "Epoch [200/200], Loss: 1.3042\n",
      "e the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the th\n"
     ]
    }
   ],
   "source": [
    "class CharRNN(CharRNN):\n",
    "    def predict_argmax(self, context_tensor, n_predictions):\n",
    "        # Apply the forward pass for the context tensor\n",
    "        # Then, store the last prediction and last hidden state\n",
    "        # YOUR CODE HERE\n",
    "        out, hid = self.forward(context_tensor)\n",
    "        self.last_out = out[-1]\n",
    "        self.last_hid = hid[-1].unsqueeze(-1)\n",
    "        \n",
    "        # Use the last prediction and last hidden state as inputs to the next forward pass\n",
    "        # Do this in a loop to predict the next `n_predictions` characters\n",
    "        # YOUR CODE HERE\n",
    "        predictions = []\n",
    "        for i in range(n_predictions):\n",
    "            str_out = itoc[np.argmax(F.softmax(self.last_out, -1).detach().numpy())]\n",
    "            predictions.append(str_out)\n",
    "            self.last_out = text_to_tensor(str_out, ctoi)\n",
    "            log, hid = self.forward(self.last_out, self.last_hid.view(1, self.hidden_dim))\n",
    "            self.last_out = log\n",
    "            self.last_hid = hid.unsqueeze(-1)  \n",
    "\n",
    "\n",
    "        return predictions\n",
    "            \n",
    "        \n",
    "\n",
    "# Initialize a model and make it overfit as above\n",
    "# Then, verify your overfitting by predicting characters given some context\n",
    "# YOUR CODE HERE\n",
    "\n",
    "input_seq = text_data[:1000]\n",
    "target_seq = text_data[1:1001]\n",
    "\n",
    "target_seq = one_hot(input_seq, target_seq, vocab_size, ctoi)\n",
    "input_seq = text_to_tensor(input_seq, ctoi)\n",
    "\n",
    "embedding_dim = 15\n",
    "hidden_dim = 30\n",
    "\n",
    "model = CharRNN(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "train_overfit(model, input_seq, target_seq)\n",
    "\n",
    "predictions = model.predict_argmax(text_to_tensor(\"Fight\", ctoi), 200)\n",
    "\n",
    "predictions = \"\".join(predictions)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e9b8b-f952-43af-a2f0-084451d85759",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25bd5ff990e4cf55e89b26541d0acf34",
     "grade": true,
     "grade_id": "cell-b783299fd35282d3",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f5d3e9-9a68-48b8-a549-0fc2b9f2b2ff",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f34d247477f051d257bc1337cfc611fa",
     "grade": false,
     "grade_id": "cell-52baebc1e4eb464c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Using the argmax function to predict the next character can yield a deterministic generator always predicting the same characters. Instead, it is common to predict the next character by sampling from the distribution of output predictions, adding some randomness into the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5701d4df-dca5-4884-8ac2-c8efe4fe4641",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cefa534edd726def1328ea0b48ed29d",
     "grade": false,
     "grade_id": "cell-e85a5e3954f17ad2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**(Question)** Implement a `predict_proba` method for your `RNN` model. It should be very similar to `predict_argmax`, but instead of using argmax, it should randomly sample from the output predictions. To do that, you can use the `torch.distribution.Categorical` class and its `sample()` method. Verify that your method correctly added some randomness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "72da1efb-39b8-496d-9c8f-cea241c41364",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ba3d32edc8f535eb8923fb8e71c9fe4",
     "grade": true,
     "grade_id": "rnn-sample",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 3.1546\n",
      "Epoch [20/200], Loss: 2.6569\n",
      "Epoch [30/200], Loss: 2.4382\n",
      "Epoch [40/200], Loss: 2.2633\n",
      "Epoch [50/200], Loss: 2.1354\n",
      "Epoch [60/200], Loss: 2.0413\n",
      "Epoch [70/200], Loss: 1.9671\n",
      "Epoch [80/200], Loss: 1.9018\n",
      "Epoch [90/200], Loss: 1.8421\n",
      "Epoch [100/200], Loss: 1.7861\n",
      "Epoch [110/200], Loss: 1.7327\n",
      "Epoch [120/200], Loss: 1.6805\n",
      "Epoch [130/200], Loss: 1.6305\n",
      "Epoch [140/200], Loss: 1.6209\n",
      "Epoch [150/200], Loss: 1.5637\n",
      "Epoch [160/200], Loss: 1.5101\n",
      "Epoch [170/200], Loss: 1.4690\n",
      "Epoch [180/200], Loss: 1.4521\n",
      "Epoch [190/200], Loss: 1.4015\n",
      "Epoch [200/200], Loss: 1.5252\n",
      "óe .Ie.8en, royeon in orysrorf\n",
      " find seay û YhkUil=lis and of ton s toWns N9o\n",
      "n  ondo- Rinw wiin lo3nes  Nn- them\n",
      "\n",
      " f í celewf Moryer warelacl to dornd te mhecale and lenuus in alrewe táe in Wlet( _ar\n"
     ]
    }
   ],
   "source": [
    "class CharRNN(CharRNN):\n",
    "    def predict_proba(self, input_context, n_predictions):\n",
    "        # YOUR CODE HERE\n",
    "        out, hid = self.forward(input_context)\n",
    "        self.last_out = out[-1]\n",
    "        self.last_hid = hid[-1].unsqueeze(-1)\n",
    "        predictions = []\n",
    "        for i in range(n_predictions):\n",
    "            proba = F.softmax(self.last_out, -1)          \n",
    "            dist = torch.distributions.categorical.Categorical(probs=proba, logits=None, validate_args=None)\n",
    "            str_out = itoc[dist.sample().item()]\n",
    "            predictions.append(str_out)\n",
    "            self.last_out = text_to_tensor(str_out, ctoi)\n",
    "            log, hid = self.forward(self.last_out, self.last_hid.view(1, self.hidden_dim))\n",
    "            self.last_out = log\n",
    "            self.last_hid = hid.unsqueeze(-1)  \n",
    "\n",
    "\n",
    "        return predictions\n",
    "\n",
    "# Verify that your predictions are not deterministic anymore\n",
    "# YOUR CODE HERE\n",
    "input_seq = text_data[:1000]\n",
    "target_seq = text_data[1:1001]\n",
    "\n",
    "target_seq = one_hot(input_seq, target_seq, vocab_size, ctoi)\n",
    "input_seq = text_to_tensor(input_seq, ctoi)\n",
    "\n",
    "embedding_dim = 15\n",
    "hidden_dim = 30\n",
    "\n",
    "model = CharRNN(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "train_overfit(model, input_seq, target_seq)\n",
    "\n",
    "predictions = model.predict_proba(text_to_tensor(\"Fight\", ctoi), 200)\n",
    "\n",
    "predictions = \"\".join(predictions)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9912f5cc-3627-41e8-b5ef-6d30f7c4868a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e56bfe8d33a343e270cfe35720aeea26",
     "grade": false,
     "grade_id": "cell-6389d46b2b8abaa0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 3. Train the RNN model on text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8024df30-af42-4ca9-be0e-16614ead56cb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44f87a393c4ae266b59141953d170a7e",
     "grade": false,
     "grade_id": "rnn-train",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Adapt your previous code to implement a proper training loop for a text dataset. To do so, we need to specify a sequence length `seq_len`, acting similarly to the batch size in classic neural networks. Then, you can either randomly sample sequences of length `seq_len` from the text dataset over `n_iters` iterations, or properly loop over the text dataset for `n_epochs` epochs (with a random starting point for each epoch to ensure different sequences), to make sure the whole dataset is seen by the model. Feel free to adjust training and model parameters empirically. Start with a small model and a small subset of the text dataset, then move on to larger experiments. Remember to use GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f65d237e-d095-45c4-ab12-6307a5bda255",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "17faecb7751e77b7fe8cae66820687ea",
     "grade": true,
     "grade_id": "cell-a4695fabcf78e1a8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 2.4217\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb Cell 29\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb#X40sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m         \u001b[39mif\u001b[39;00m((i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m%\u001b[39m\u001b[39m10\u001b[39m\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb#X40sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m             \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m, n_iters, loss\u001b[39m.\u001b[39mitem()))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb#X40sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m train_overfit2(model, input_seqs, target_seqs, n_iters\u001b[39m=\u001b[39;49mn_iters, learning_rate\u001b[39m=\u001b[39;49mlearning_rate)\n",
      "\u001b[1;32m/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb Cell 29\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb#X40sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_iters):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb#X40sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m     \u001b[39mfor\u001b[39;00m k, seq \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(input_seq):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb#X40sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m         outputs, _ \u001b[39m=\u001b[39m model(seq)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb#X40sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m         target \u001b[39m=\u001b[39m target_seq[k]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb#X40sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         \u001b[39m# print(outputs)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb#X40sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m         \u001b[39m# print(target)\u001b[39;00m\n",
      "File \u001b[0;32m~/3A/mldl/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/3A/mldl/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb#X40sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb#X40sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb(tensor_data)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb#X40sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m out, hid \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(out, hidden_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb#X40sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin(out)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/faroukso/3A/s9/IS319-Deep-Learning/tp3/text-datasets/dl-tp3-fs.ipynb#X40sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out, hid\n",
      "File \u001b[0;32m~/3A/mldl/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/3A/mldl/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/3A/mldl/lib/python3.10/site-packages/torch/nn/modules/rnn.py:553\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRNN_TANH\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 553\u001b[0m         result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mrnn_tanh(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    554\u001b[0m                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional,\n\u001b[1;32m    555\u001b[0m                               \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    556\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    557\u001b[0m         result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mrnn_relu(\u001b[39minput\u001b[39m, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers,\n\u001b[1;32m    558\u001b[0m                               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional,\n\u001b[1;32m    559\u001b[0m                               \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the text dataset, compute its mappings and convert it to tensor\n",
    "# YOUR CODE HERE\n",
    "import random\n",
    "\n",
    "# def one_hot(input_sequences, target_sequences, vocab_size, ctoi):\n",
    "#     one_hot_vecs = []\n",
    "#     for j, input_seq in enumerate(input_sequences):\n",
    "#         one_hot_vec = np.zeros((len(input_seq), vocab_size))\n",
    "#         for i, c in enumerate(input_seq):\n",
    "#             t_index = ctoi[target_sequences[j][i]]\n",
    "#             one_hot_vec[i][t_index] = 1\n",
    "#         one_hot_vecs.append(torch.Tensor(one_hot_vec))\n",
    "#     return torch.stack(one_hot_vecs)\n",
    "\n",
    "\n",
    "text_data = open(text_data_fname, 'r').read()\n",
    "data_length = len(text_data)\n",
    "\n",
    "vocabulary = list(set(text_data))\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "ctoi = {vocabulary[i] : i for i in range(vocab_size)}\n",
    "itoc = {i : vocabulary[i] for i in range(vocab_size)}\n",
    "\n",
    "seq_len = 10\n",
    "num_batches = 200\n",
    "start_indexes = random.sample(range(0, data_length+1), num_batches)\n",
    "\n",
    "input_seqs = np.array([[text_data[i:i+seq_len], text_data[i+1:i+seq_len+1]] for i in start_indexes])\n",
    "\n",
    "target_seqs = [one_hot(x[0], x[1], vocab_size, ctoi) for x in input_seqs]\n",
    "input_seqs = [text_to_tensor(x, ctoi) for x in input_seqs[:,0]]\n",
    "\n",
    "# Initialize training parameters\n",
    "# YOUR CODE HERE\n",
    "\n",
    "embedding_dim = 10\n",
    "hidden_dim = 50\n",
    "num_layers = 5\n",
    "learning_rate = 0.05\n",
    "n_iters = 100\n",
    "\n",
    "\n",
    "# Initialize a character-level RNN model\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "model = CharRNN(vocab_size, embedding_dim, hidden_dim, num_layers=num_layers)\n",
    "    \n",
    "# Setup the training loop\n",
    "# Regularly record the loss and sample from the model to monitor what is happening\n",
    "# YOUR CODE HERE\n",
    "\n",
    "def train_overfit2(model, input_seq, target_seq, n_iters=200, learning_rate=0.2):\n",
    "    # YOUR CODE HERE\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0.005, momentum = 0.9)  \n",
    "\n",
    "    for i in range(n_iters):\n",
    "        for k, seq in enumerate(input_seq):\n",
    "            outputs, _ = model(seq)\n",
    "            target = target_seq[k]\n",
    "            # print(outputs)\n",
    "            # print(target)\n",
    "            loss = criterion(outputs, target.squeeze(1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if((i+1)%10==0):\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(i+1, n_iters, loss.item()))\n",
    "\n",
    "\n",
    "train_overfit2(model, input_seqs, target_seqs, n_iters=n_iters, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a318abd-5c3c-462d-9944-9aec1f78446c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "113032562a0b82e504d636abf3164360",
     "grade": false,
     "grade_id": "rnn-predict",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** From your trained model, play around with its predictions: start with a custom input sequence and ask the model to predict the rest. Analyze and comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "53d516b1-477c-4237-956a-471062dd6019",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bd724d3a1f3ba5d0b965b9cf7905160",
     "grade": true,
     "grade_id": "cell-08bfe03b817a9908",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heatergged1;ed,' o LerrenrÉaf\n",
      "  '`e simelly 'oilarle:.',` W lSey..\n",
      " alerltwgd inen.''  T GA=dkid `verhhe.' holklegelenR' 'pzenken.'Zileeelgensogherd7eldankd iny IÉarrhrXer-, youte\trkelL,' wouvl2dj êir\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "predictions = model.predict_proba(text_to_tensor(\"Fight\", ctoi), 200)\n",
    "predictions = \"\".join(predictions)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02a4a5-5aab-403a-9bd3-fd752dc1dc9a",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "288f4746f3c7d1906ca99b35f4a6a6e3",
     "grade": true,
     "grade_id": "cell-6b41d47e15ea128a",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de32122-8819-4d6b-8f7a-e96f671311d6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2eddbf9984d6a4d51a7ea1301800bdf3",
     "grade": false,
     "grade_id": "cell-a69a65798f792cfc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 4. Experiment with different RNN architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5def498-9119-45fd-8807-8260cd0a05d8",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "977d70928beb993f5bc6323199b1e363",
     "grade": false,
     "grade_id": "rnn-experiments",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": true
    }
   },
   "source": [
    "**(Question)** Experiment with different RNN architecures. Potential ideas are multi-layer RNNs, GRUs and LSTMs. All models can be extended to multi-layer using the `num_layers` parameter. Analyze and comment your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c6fe7d-d159-401d-9c7c-f7e9a5b86cba",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "77ebb731623feb1292e7758788ad56d4",
     "grade": true,
     "grade_id": "cell-7bbcfb8355f44b5d",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13329eaa-6a48-47cf-912d-424a79680f91",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5bffac2e6bdfed739aea204b3c40a792",
     "grade": true,
     "grade_id": "cell-3961b7f97f038a4b",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
