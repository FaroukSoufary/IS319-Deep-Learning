{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "abb49b5c-7533-4bf0-bcce-37373d6fa072",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "abb49b5c-7533-4bf0-bcce-37373d6fa072",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d1502bbeee981ecc3a7a4464df92be0c",
          "grade": false,
          "grade_id": "cell-a975d7bfb06f30ae",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "# IS319 - Deep Learning\n",
        "\n",
        "## TP3 - Recurrent neural networks\n",
        "\n",
        "Credits: Andrej Karpathy\n",
        "\n",
        "The goal of this TP is to experiment with recurrent neural networks for a character-level language model to generate text that looks like training text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "16d03ccd-089a-4553-bd96-7bd0c0f71949",
      "metadata": {
        "id": "16d03ccd-089a-4553-bd96-7bd0c0f71949"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4c736b5-43df-4aca-9ba3-dd34ed8366f4",
      "metadata": {
        "id": "c4c736b5-43df-4aca-9ba3-dd34ed8366f4"
      },
      "source": [
        "## 1. Text data preprocessing\n",
        "\n",
        "Several text datasets are provided, feel free to experiment with different ones throughout the TP. At the beginning, use a small subset of a given dataset (for example use only 10k characters)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "LdINbAH3tFYw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdINbAH3tFYw",
        "outputId": "32b26b47-e05e-472e-94b6-9fb654cae885"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "tar: Error opening archive: Failed to open 'text-datasets.tgz'\n"
          ]
        }
      ],
      "source": [
        "!tar -xvf text-datasets.tgz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "e56a81b9-b733-4d87-b8d7-2184d5e5c2de",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e56a81b9-b733-4d87-b8d7-2184d5e5c2de",
        "outputId": "2b992603-fe62-4cbc-d0cf-c927c8c649a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset `baudelaire.txt` contains 10000 characters.\n",
            "Excerpt of the dataset:\n",
            "LES FLEURS DU MAL\n",
            "\n",
            "par\n",
            "\n",
            "CHARLES BAUDELAIRE\n",
            "\n",
            "\n",
            "AU LECTEUR\n",
            "\n",
            "\n",
            "La sottise, l'erreur, le péché, la lésine,\n",
            "Occupent nos esprits et travaillent nos corps,\n",
            "Et nous alimentons nos aimables remords,\n",
            "Comme les mendiants nourrissent leur vermine.\n",
            "\n",
            "Nos péchés sont têtus, nos repentirs sont lâches,\n",
            "Nous nous faisons payer grassement nos aveux,\n",
            "Et nous rentrons gaîment dans le chemin bourbeux,\n",
            "Croyant par de vils pleurs laver toutes nos taches.\n",
            "\n",
            "Sur l'oreiller du mal c'est Satan Trismégiste\n",
            "Qui berce longuement notre esprit enchanté,\n",
            "Et le riche métal de notre volonté\n",
            "Est tout vaporisé par ce savant chimiste.\n",
            "\n",
            "C'est le Diable qui tient les fils qui nous remuent!\n",
            "Aux objets répugnants nous trouvons des appas;\n",
            "Chaque jour vers l'Enfer nous descendons d'un pas,\n",
            "Sans horreur, à travers des ténèbres qui puent.\n",
            "\n",
            "Ainsi qu'un débauché pauvre qui baise et mange\n",
            "Le sein martyrisé d'une antique catin,\n",
            "Nous volons au passage un plaisir clandestin\n",
            "Que nous pressons bien fort comme une vieille orange.\n",
            "\n",
            "Serré, fourmillant, comme un million d'helminthes,\n",
            "Dans nos cerveaux ribote un peuple de Démons,\n",
            "Et, quand nous respirons, la Mort dans nos poumons\n",
            "Descend, fleuve invisible, avec de sourdes plaintes.\n",
            "\n",
            "Si le viol, le poison, le poignard, l'incendie,\n",
            "N'ont pas encore brodé de leurs plaisants desseins\n",
            "Le canevas banal de nos piteux destins,\n",
            "C'est que notre âme, hélas! n'est pas assez hardie.\n",
            "\n",
            "Mais parmi les chacals, les panthères, les lices,\n",
            "Les singes, les scorpions, les vautours, les serpents,\n",
            "Les monstres glapissants, hurlants, grognants, rampants\n",
            "Dans la ménagerie infâme de nos vices,\n",
            "\n",
            "Il en est un plus laid, plus méchant, plus immonde!\n",
            "Quoiqu'il ne pousse ni grands gestes ni grands cris,\n",
            "Il ferait volontiers de la terre un débris\n",
            "Et dans un bâillement avalerait le monde;\n",
            "\n",
            "C'est l'Ennui!--L'oeil chargé d'un pleur involontaire,\n",
            "Il rêve d'échafauds en fumant son houka.\n",
            "Tu le connais, lecteur, ce monstre délicat,\n",
            "--Hypocrite lecteur,--mon semblable,--mon frère!\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "SPLEEN ET IDÉAL\n",
            "\n",
            "BENEDICTION\n",
            "\n",
            "\n",
            "L\n"
          ]
        }
      ],
      "source": [
        "text_data_fname = 'baudelaire.txt'  # ~0.1m characters (French)\n",
        "# text_data_fname = 'proust.txt'      # ~7.3m characters (French)\n",
        "# text_data_fname = 'shakespeare.txt' # ~0.1m characters (English)\n",
        "# text_data_fname = 'lotr.txt'        # ~2.5m characters (English)\n",
        "# text_data_fname = 'doom.c'          # ~1m characters (C Code)\n",
        "# text_data_fname = 'linux.c'         # ~11.5m characters (C code)\n",
        "\n",
        "text_data = open(text_data_fname, 'r',encoding=\"utf8\").read()\n",
        "text_data = text_data[:10000] # use a small subset\n",
        "print(f'Dataset `{text_data_fname}` contains {len(text_data)} characters.')\n",
        "print('Excerpt of the dataset:')\n",
        "print(text_data[:2000])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9b9a404-8b09-4ab1-9529-98ff57650ef5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "d9b9a404-8b09-4ab1-9529-98ff57650ef5",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d6b030743489f86ece30d79835434297",
          "grade": false,
          "grade_id": "cell-9695c6f2cf95337c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "**(Question)** Create a character-level vocabulary for your text data. Create two dictionaries: `ctoi` mapping each character to an index, and the reverse `itoc` mapping each index to its corresponding character. Implement the functions to convert text to tensor and tensor to text using these mappings. Apply these functions to some text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "885ecc8a-c654-463b-9353-f7a18a607199",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "deletable": false,
        "id": "885ecc8a-c654-463b-9353-f7a18a607199",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e3eb46710b4006993f4eb9c557a2376f",
          "grade": true,
          "grade_id": "vocabulary",
          "locked": false,
          "points": 1,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "2d836c27-9154-454d-b3b0-74ee9efeb0bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'z': 0, 'J': 1, '?': 2, 'H': 3, 'ç': 4, 'y': 5, 'N': 6, 'I': 7, 'c': 8, 'n': 9, 'L': 10, 'i': 11, 'b': 12, 'î': 13, 'k': 14, 'o': 15, 'E': 16, 'à': 17, 'D': 18, 'g': 19, 'R': 20, 'P': 21, 'l': 22, 'é': 23, ';': 24, 'û': 25, 'Q': 26, '!': 27, \"'\": 28, 'ù': 29, ' ': 30, 'â': 31, 'j': 32, 's': 33, '.': 34, 'è': 35, 'd': 36, 'É': 37, '»': 38, 'A': 39, 'v': 40, 'B': 41, 'O': 42, 'f': 43, ',': 44, 'q': 45, 'ê': 46, 't': 47, 'G': 48, '«': 49, 'W': 50, 'ô': 51, 'h': 52, 'e': 53, 'x': 54, '-': 55, 'S': 56, 'U': 57, 'F': 58, 'p': 59, 'a': 60, '\\n': 61, 'r': 62, 'C': 63, 'M': 64, 'u': 65, 'm': 66, ':': 67, 'V': 68, '_': 69, 'T': 70}\n",
            "{0: 'z', 1: 'J', 2: '?', 3: 'H', 4: 'ç', 5: 'y', 6: 'N', 7: 'I', 8: 'c', 9: 'n', 10: 'L', 11: 'i', 12: 'b', 13: 'î', 14: 'k', 15: 'o', 16: 'E', 17: 'à', 18: 'D', 19: 'g', 20: 'R', 21: 'P', 22: 'l', 23: 'é', 24: ';', 25: 'û', 26: 'Q', 27: '!', 28: \"'\", 29: 'ù', 30: ' ', 31: 'â', 32: 'j', 33: 's', 34: '.', 35: 'è', 36: 'd', 37: 'É', 38: '»', 39: 'A', 40: 'v', 41: 'B', 42: 'O', 43: 'f', 44: ',', 45: 'q', 46: 'ê', 47: 't', 48: 'G', 49: '«', 50: 'W', 51: 'ô', 52: 'h', 53: 'e', 54: 'x', 55: '-', 56: 'S', 57: 'U', 58: 'F', 59: 'p', 60: 'a', 61: '\\n', 62: 'r', 63: 'C', 64: 'M', 65: 'u', 66: 'm', 67: ':', 68: 'V', 69: '_', 70: 'T'}\n",
            "71\n",
            "tensor([10, 16, 56, 30, 58, 10, 16, 57, 20, 56])\n",
            "LES FLEURS\n"
          ]
        }
      ],
      "source": [
        "# Create the vocabulary and the two mapping dictionaries\n",
        "# YOUR CODE HERE\n",
        "import numpy as np\n",
        "\n",
        "idx = 0\n",
        "ctoi = {}\n",
        "itoc = {}\n",
        "\n",
        "voc = set(text_data)\n",
        "\n",
        "for elt in voc:\n",
        "    ctoi[elt] = idx\n",
        "    itoc[idx] = elt\n",
        "    idx += 1\n",
        "\n",
        "print(ctoi)\n",
        "print(itoc)\n",
        "print(len(ctoi))\n",
        "# Implement the function converting text to tensor\n",
        "def text_to_tensor(text, ctoi):\n",
        "    # YOUR CODE HERE\n",
        "    return torch.LongTensor(np.array([ctoi[c] for c in text]))\n",
        "\n",
        "\n",
        "# Implement the function converting tensor to text\n",
        "def tensor_to_text(tensor, itoc):\n",
        "    # YOUR CODE HERE\n",
        "    return ''.join([itoc[elt.item()] for elt in tensor])#torch.argmax(tensor, dim=2)])\n",
        "\n",
        "# Apply your functions to some text data\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()\n",
        "a = text_to_tensor(text_data[:10], ctoi)\n",
        "\n",
        "print(a)\n",
        "print(tensor_to_text(a, itoc))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8da1b7f-549d-45d9-8fea-309ee0e76a90",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b8da1b7f-549d-45d9-8fea-309ee0e76a90",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "fbfd5a4df6e6081581ad38d7180fddfb",
          "grade": false,
          "grade_id": "cell-172b8befc4633227",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 2. Setup a character-level recurrent neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d33cfac-93f7-4980-8510-fcf675d3a06b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3d33cfac-93f7-4980-8510-fcf675d3a06b",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6cb30929efdd10eb6066750f4b94bf14",
          "grade": false,
          "grade_id": "embedding",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": true
        }
      },
      "source": [
        "**(Question)** Setup a simple embedding layer with `nn.Embedding` to project character indices to `embedding_dim` dimensional vectors. Explain precisely how this layer works and what are its outputs for a given input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "id": "609747a0-6634-4232-92cb-72946ef516b0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "deletable": false,
        "id": "609747a0-6634-4232-92cb-72946ef516b0",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a9dd84493589d73c13fac37411610a2e",
          "grade": true,
          "grade_id": "cell-a3b5adeb8111779b",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "ce1b75e1-b68f-4656-e3eb-7e516e65f093"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "embedding_dim = 5\n",
        "vocab_size = len(ctoi)\n",
        "embedding = nn.Embedding(vocab_size,embedding_dim)\n",
        "\n",
        "emb = embedding(text_to_tensor(text_data[:8], ctoi))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26f7a5e3-f137-49ef-a135-a855029b724b",
      "metadata": {
        "deletable": false,
        "id": "26f7a5e3-f137-49ef-a135-a855029b724b",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "ba287efa3849a2ddcb98b94b287fd199",
          "grade": true,
          "grade_id": "cell-4065672821a5801f",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "The nn.Embedding layer is designed to embed categorical variables, such as characters or words within a vocabulary, into continuous vectors of a specified dimension (embedding_dim). In the context of an input sequence, the layer transforms the sequence of indices into a tensor of size (len(input_seq), embedding_dim). Each row of this tensor represents the embedding of the corresponding index in the input sequence, projecting it into an embedding_dim-dimensional space.\n",
        "\n",
        "In the example above, the initial representation is randomized. However, when a learning model is applied, the weights of this layer are adjusted through training to capture textual relationships that cannot be represented using inddices in the vocabulary. As the model learns, the embedding vectors adapt to represent semantic connections between the inputs. For instance, characters or words that frequently co-occur in the training set will have proximate representations in the embedding space.\n",
        "\n",
        "This dynamic adjustment of weights during training allows the nn.Embedding layer to create embeddings that encode meaningful relationships between categorical inputs, enhancing the model's ability to understand and generalize from the input data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ebc7b1c-6469-4c63-9965-7a7f39734302",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7ebc7b1c-6469-4c63-9965-7a7f39734302",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6897eae892e4aa50cceac9bca0a83f82",
          "grade": false,
          "grade_id": "base-rnn",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": true
        }
      },
      "source": [
        "**(Question)** Setup a single-layer RNN with `nn.RNN` (without defining a custom class). Use `hidden_dim` size for hidden states. Explain precisely the outputs of this layer for a given input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "id": "162dd7af-cf96-4279-b512-1507433c7826",
      "metadata": {
        "deletable": false,
        "id": "162dd7af-cf96-4279-b512-1507433c7826",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "095b1c904c7c5fa3d5371de208f7300a",
          "grade": true,
          "grade_id": "cell-c0b3cdd14603b6d1",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([8, 20])\n",
            "torch.Size([2, 20])\n"
          ]
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "hidden_dim = 20\n",
        "rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=2)\n",
        "output, hidden = rnn(emb)\n",
        "\n",
        "print(output.size())\n",
        "print(hidden.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56780e84-8767-4941-ba0a-2047b106fc35",
      "metadata": {
        "deletable": false,
        "id": "56780e84-8767-4941-ba0a-2047b106fc35",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2f3230528c7fabd5b26931934ac4e8b0",
          "grade": true,
          "grade_id": "cell-9c6c3e3359e2c37e",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "For a given input sequence, the nn.RNN layer returns two tensors. The first tensor, named 'output', is of size (len(input), hidden_size). This tensor stores the output of the RNN layer at each time step in the input sequence. Each row of the 'output' tensor represents the hidden state of the RNN at a specific time step.\n",
        "\n",
        "The second tensor, named 'hidden', is of size (num_layers, hidden_dim). This tensor stores the final hidden state of the RNN. In the context of 'hidden' contains the summarization of information from the entire input sequence in each layer of the RNN. It serves as a compact representation that captures the essential features learned by the RNN during the processing of the input sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76df8c7a-2a3d-43a0-9666-a797d1a3dbca",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "76df8c7a-2a3d-43a0-9666-a797d1a3dbca",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "3993b8b02d0be0e16de189c4850bbfce",
          "grade": false,
          "grade_id": "rnn-model",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": true
        }
      },
      "source": [
        "**(Question)** Create a simple RNN model with a custom `nn.Module` class. It should contain: an embedding layer, a single-layer RNN, and a dense output layer. For each character of the input sequence, the model should predict the probability of the next character. The forward method should return the probabilities for next characters and the corresponding hidden states.\n",
        "After completing the class, create a model and apply the forward pass on some input text. Understand and explain the results.\n",
        "\n",
        "*Note:* depending on how you implement the loss function later, it can be convenient to return logits instead of probabilities, i.e. raw values of the output layer before any activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "id": "6cc835a8-a05f-4f78-b50e-4edbdb04305f",
      "metadata": {
        "deletable": false,
        "id": "6cc835a8-a05f-4f78-b50e-4edbdb04305f",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "b587c6f505f5d3719869ae8a57fcb5c6",
          "grade": true,
          "grade_id": "cell-e55d41dd89bcf74f",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "L\n",
            "torch.Size([1, 71])\n"
          ]
        }
      ],
      "source": [
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
        "        '''Initialize model parameters and layers.'''\n",
        "        super().__init__()\n",
        "        # YOUR CODE HERE\n",
        "        self.embedding = nn.Embedding(vocab_size,embedding_dim)\n",
        "        self.rnn = nn.RNN(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers)\n",
        "        self.dense = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, tensor_data, hidden_state=None):\n",
        "        '''Apply the forward pass for some text data already converted to tensor.'''\n",
        "        # YOUR CODE HERE\n",
        "        embedding = self.embedding(tensor_data)\n",
        "        output, hidden = self.rnn(embedding, hidden_state)\n",
        "        logits = self.dense(output)\n",
        "        return logits, hidden\n",
        "\n",
        "# Initialize a model and apply the forward pass on some input text\n",
        "# YOUR CODE HERE\n",
        "\n",
        "\n",
        "vocab_size = len(ctoi)\n",
        "embedding_dim = 200\n",
        "hidden_dim = 100\n",
        "\n",
        "charRNN = CharRNN(vocab_size,embedding_dim,hidden_dim)\n",
        "logits, _ = charRNN.forward(text_to_tensor(text_data[:1],ctoi))\n",
        "\n",
        "print(text_data[0])\n",
        "print(logits.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41614741-8bf2-4ead-8205-788623404a27",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "41614741-8bf2-4ead-8205-788623404a27",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9bb6ca5b49b7c8b46253cf2a7f1200c5",
          "grade": false,
          "grade_id": "rnn-overfit",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "**(Question)** Implement a simple training loop to overfit on a small input sequence. The loss function should be a categorical cross entropy on the predicted characters. Monitor the loss function value over the iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "474f97eb-685f-41c1-8249-1eb4df9e168c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "deletable": false,
        "id": "474f97eb-685f-41c1-8249-1eb4df9e168c",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "fa15210da5aabdca4ecf3228efec88be",
          "grade": true,
          "grade_id": "cell-1904f4989149b1ef",
          "locked": false,
          "points": 3,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "36089518-13b4-4dc8-8cea-4f9c680adb96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([10, 16, 56, 30, 58, 10, 16, 57]) tensor([56])\n",
            "torch.Size([8]) torch.Size([8])\n",
            "Iteration 10/200, Loss: 2.2388200759887695\n",
            "Iteration 20/200, Loss: 0.18988797068595886\n",
            "Iteration 30/200, Loss: 0.04161001369357109\n",
            "Iteration 40/200, Loss: 0.009962371550500393\n",
            "Iteration 50/200, Loss: 0.004840867128223181\n",
            "Iteration 60/200, Loss: 0.003572132671251893\n",
            "Iteration 70/200, Loss: 0.003282556077465415\n",
            "Iteration 80/200, Loss: 0.0032899973448365927\n",
            "Iteration 90/200, Loss: 0.0034065735526382923\n",
            "Iteration 100/200, Loss: 0.003574709640815854\n",
            "Iteration 110/200, Loss: 0.003773884382098913\n",
            "Iteration 120/200, Loss: 0.003994535654783249\n",
            "Iteration 130/200, Loss: 0.004231789615005255\n",
            "Iteration 140/200, Loss: 0.004481961950659752\n",
            "Iteration 150/200, Loss: 0.0047417981550097466\n",
            "Iteration 160/200, Loss: 0.005008433014154434\n",
            "Iteration 170/200, Loss: 0.005278781056404114\n",
            "Iteration 180/200, Loss: 0.00554983364418149\n",
            "Iteration 190/200, Loss: 0.005818541627377272\n",
            "Iteration 200/200, Loss: 0.006082095205783844\n"
          ]
        }
      ],
      "source": [
        "# Sample a small input sequence into tensor `input_seq` and store its corresponding expected sequence into tensor `target_seq`\n",
        "# YOUR CODE HERE\n",
        "vocab_size = len(ctoi)\n",
        "embedding_dim = 20\n",
        "hidden_dim = 100\n",
        "input_seq = text_to_tensor(text_data[:10], ctoi)\n",
        "input_seq, last_seq = input_seq[:-2], input_seq[-1:]\n",
        "print(input_seq, last_seq)\n",
        "\n",
        "target_seq = torch.cat([input_seq[1:], last_seq])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(input_seq.shape, target_seq.shape)\n",
        "\n",
        "# Implement a training loop overfitting an input sequence and monitoring the loss function\n",
        "def train_overfit(model, input_seq, target_seq, n_iters=200, learning_rate=0.02):\n",
        "    # YOUR CODE HERE\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate, weight_decay=5e-3, momentum=0.9)\n",
        "    hidden = None\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "      logits, hidden = model.forward(input_seq, hidden)\n",
        "      hidden = hidden.detach() #Once we update the hidden state we need to detach it, to not backpropagate through it in the next batch\n",
        "      #output = F.softmax(logits, dim=1)\n",
        "      loss = criterion(logits, target_seq)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if iter % 10 == 0:\n",
        "          print(f'Iteration {iter}/{n_iters}, Loss: {loss.item()}')\n",
        "\n",
        "\n",
        "\n",
        "# Initialize a model and make it overfit the input sequence\n",
        "# YOUR CODE HERE\n",
        "\n",
        "charRNN = CharRNN(vocab_size, embedding_dim, hidden_dim)\n",
        "train_overfit(charRNN, input_seq, target_seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68c66f34-7afb-4920-b6d2-5642d28a0770",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "68c66f34-7afb-4920-b6d2-5642d28a0770",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9cd6d0a90166cea96ce250f8d8f2e083",
          "grade": false,
          "grade_id": "rnn-argmax",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": true
        }
      },
      "source": [
        "**(Question)** Implement a `predict_argmax` method for your `RNN` model. Then, verify your overfitting: use some characters of your input sequence as context to predict the remaining ones. Experiment with the current model and analyze the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "id": "4d0350ae-a23a-4802-a2a5-012193ede15b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "deletable": false,
        "id": "4d0350ae-a23a-4802-a2a5-012193ede15b",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "0114ed939e7dd8b656bd0f03bd576a89",
          "grade": true,
          "grade_id": "cell-2d706c010aeccb0d",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "outputId": "615e47b2-8178-4d39-cb7b-ade0f6c71a1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3, 3, 8, 7, 4, 8, 6, 3, 0, 5]) tensor([2, 3, 3, 8, 7, 4, 8, 6, 3, 0, 5, 7])\n",
            "Iteration 10/200, Loss: 1.6787503957748413\n",
            "Iteration 20/200, Loss: 0.9198446869850159\n",
            "Iteration 30/200, Loss: 0.4029098451137543\n",
            "Iteration 40/200, Loss: 0.18057680130004883\n",
            "Iteration 50/200, Loss: 0.09902171045541763\n",
            "Iteration 60/200, Loss: 0.06560871750116348\n",
            "Iteration 70/200, Loss: 0.04994722083210945\n",
            "Iteration 80/200, Loss: 0.04155581817030907\n",
            "Iteration 90/200, Loss: 0.03648228570818901\n",
            "Iteration 100/200, Loss: 0.03309526666998863\n",
            "Iteration 110/200, Loss: 0.030664006248116493\n",
            "Iteration 120/200, Loss: 0.028828440234065056\n",
            "Iteration 130/200, Loss: 0.027393845841288567\n",
            "Iteration 140/200, Loss: 0.026245824992656708\n",
            "Iteration 150/200, Loss: 0.025311579927802086\n",
            "Iteration 160/200, Loss: 0.024541625753045082\n",
            "Iteration 170/200, Loss: 0.02390090562403202\n",
            "Iteration 180/200, Loss: 0.023363424465060234\n",
            "Iteration 190/200, Loss: 0.022909604012966156\n",
            "Iteration 200/200, Loss: 0.02252424694597721\n",
            "hello world! \n"
          ]
        }
      ],
      "source": [
        "class CharRNN(CharRNN):\n",
        "    def predict_argmax(self, context_tensor, n_predictions):\n",
        "        # Apply the forward pass for the context tensor\n",
        "        # Then, store the last prediction and last hidden state\n",
        "        # YOUR CODE HERE\n",
        "        predictions = []\n",
        "        logits, hidden = self.forward(context_tensor)\n",
        "        output = F.softmax(logits, dim=1)[-1]\n",
        "        last_pred = torch.argmax(output)\n",
        "        last_pred = torch.LongTensor(last_pred).unsqueeze(-1)\n",
        "        predictions.append(last_pred)\n",
        "        # Use the last prediction and last hidden state as inputs to the next forward pass\n",
        "        # Do this in a loop to predict the next `n_predictions` characters\n",
        "        # YOUR CODE HERE\n",
        "        for _ in range(n_predictions):\n",
        "            logits, hidden = self.forward(last_pred, hidden)\n",
        "            output = F.softmax(logits, dim=1)\n",
        "            last_pred = torch.argmax(output).unsqueeze(-1)\n",
        "            predictions.append(last_pred)\n",
        "        return predictions\n",
        "\n",
        "overfit_data = \"hello world!\"\n",
        "target_overfit = \"ello world! \"\n",
        "voc_ = set(overfit_data)\n",
        "\n",
        "ctoi_ = {}\n",
        "itoc_ = {}\n",
        "\n",
        "idx_ = 0\n",
        "\n",
        "for elt in voc_:\n",
        "    ctoi_[elt] = idx_\n",
        "    itoc_[idx_] = elt\n",
        "    idx_ += 1\n",
        "# Initialize a model and make it overfit as above\n",
        "# Then, verify your overfitting by predicting characters given some context\n",
        "# YOUR CODE HERE\n",
        "\n",
        "vocab_size = len(ctoi_)\n",
        "embedding_dim = 20\n",
        "hidden_dim = 32\n",
        "\n",
        "\n",
        "input_seq = text_to_tensor(overfit_data, ctoi_)\n",
        "target_seq = text_to_tensor(target_overfit,ctoi_)\n",
        "\n",
        "print(input_seq, target_seq)\n",
        "charRNN = CharRNN(vocab_size, embedding_dim, hidden_dim)\n",
        "train_overfit(charRNN, input_seq, target_seq)\n",
        "\n",
        "print(\"hello\" + tensor_to_text(charRNN.predict_argmax(text_to_tensor(\"hello\",ctoi_),7),itoc_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b3e9b8b-f952-43af-a2f0-084451d85759",
      "metadata": {
        "deletable": false,
        "id": "4b3e9b8b-f952-43af-a2f0-084451d85759",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "25bd5ff990e4cf55e89b26541d0acf34",
          "grade": true,
          "grade_id": "cell-b783299fd35282d3",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "The model trained on \"hello world!\" manages to overfit on this sequence, and when asked to generate text given \"hello\" as a context, the model manages to predict \"world!\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4f5d3e9-9a68-48b8-a549-0fc2b9f2b2ff",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e4f5d3e9-9a68-48b8-a549-0fc2b9f2b2ff",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f34d247477f051d257bc1337cfc611fa",
          "grade": false,
          "grade_id": "cell-52baebc1e4eb464c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "Using the argmax function to predict the next character can yield a deterministic generator always predicting the same characters. Instead, it is common to predict the next character by sampling from the distribution of output predictions, adding some randomness into the generator."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5701d4df-dca5-4884-8ac2-c8efe4fe4641",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5701d4df-dca5-4884-8ac2-c8efe4fe4641",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9cefa534edd726def1328ea0b48ed29d",
          "grade": false,
          "grade_id": "cell-e85a5e3954f17ad2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "**(Question)** Implement a `predict_proba` method for your `RNN` model. It should be very similar to `predict_argmax`, but instead of using argmax, it should randomly sample from the output predictions. To do that, you can use the `torch.distribution.Categorical` class and its `sample()` method. Verify that your method correctly added some randomness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "argmax :  tensor(2) tensor(0.3000)\n",
            "sample distribution :  tensor(2) tensor(0.3000)\n",
            "sample distribution :  tensor(3) tensor(0.2000)\n",
            "sample distribution :  tensor(3) tensor(0.2000)\n"
          ]
        }
      ],
      "source": [
        "from torch.distributions import Categorical\n",
        "\n",
        "\n",
        "tensor = torch.tensor([0.06,0.04,0.3,0.2,0.1,0.05,0.09,0.06,0.1])\n",
        "distribution = Categorical(probs=tensor)\n",
        "last_pred1 = distribution.sample()\n",
        "last_pred2 = distribution.sample()\n",
        "last_pred3 = distribution.sample()\n",
        "argmax = tensor.argmax()\n",
        "\n",
        "\n",
        "print(\"argmax : \" ,argmax, tensor[argmax])\n",
        "print(\"sample distribution : \", last_pred1, tensor[last_pred1])\n",
        "print(\"sample distribution : \", last_pred2, tensor[last_pred2])\n",
        "print(\"sample distribution : \", last_pred3, tensor[last_pred3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "id": "72da1efb-39b8-496d-9c8f-cea241c41364",
      "metadata": {
        "deletable": false,
        "id": "72da1efb-39b8-496d-9c8f-cea241c41364",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2ba3d32edc8f535eb8923fb8e71c9fe4",
          "grade": true,
          "grade_id": "rnn-sample",
          "locked": false,
          "points": 2,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3, 3, 8, 7, 4, 8, 6, 3, 0, 5]) tensor([2, 3, 3, 8, 7, 4, 8, 6, 3, 0, 5, 7])\n",
            "Iteration 10/200, Loss: 1.8943047523498535\n",
            "Iteration 20/200, Loss: 1.4104305505752563\n",
            "Iteration 30/200, Loss: 0.9544656276702881\n",
            "Iteration 40/200, Loss: 0.6130251288414001\n",
            "Iteration 50/200, Loss: 0.3648529052734375\n",
            "Iteration 60/200, Loss: 0.22753620147705078\n",
            "Iteration 70/200, Loss: 0.15725648403167725\n",
            "Iteration 80/200, Loss: 0.11885526031255722\n",
            "Iteration 90/200, Loss: 0.09642663598060608\n",
            "Iteration 100/200, Loss: 0.08206096291542053\n",
            "Iteration 110/200, Loss: 0.07215886563062668\n",
            "Iteration 120/200, Loss: 0.06494277715682983\n",
            "Iteration 130/200, Loss: 0.05945123732089996\n",
            "Iteration 140/200, Loss: 0.055133964866399765\n",
            "Iteration 150/200, Loss: 0.05165635421872139\n",
            "Iteration 160/200, Loss: 0.048802196979522705\n",
            "Iteration 170/200, Loss: 0.04642455652356148\n",
            "Iteration 180/200, Loss: 0.044419318437576294\n",
            "Iteration 190/200, Loss: 0.0427105575799942\n",
            "Iteration 200/200, Loss: 0.04124153032898903\n",
            "hello world! \n"
          ]
        }
      ],
      "source": [
        "class CharRNN(CharRNN):\n",
        "    def predict_proba(self, input_context, n_predictions):\n",
        "        # YOUR CODE HERE\n",
        "        predictions = []\n",
        "        logits, hidden = self.forward(input_context)\n",
        "        output = F.softmax(logits, dim=1)[-1]\n",
        "        from torch.distributions import Categorical\n",
        "        distribution = Categorical(probs=output)\n",
        "        last_pred = distribution.sample()\n",
        "        #max_pred = torch.argmax(output)\n",
        "        #print(last_pred, max_pred, max(output))\n",
        "        last_pred = torch.LongTensor(last_pred).unsqueeze(-1)\n",
        "        predictions.append(last_pred)\n",
        "        # Use the last prediction and last hidden state as inputs to the next forward pass\n",
        "        # Do this in a loop to predict the next `n_predictions` characters\n",
        "        # YOUR CODE HERE\n",
        "        for _ in range(n_predictions):\n",
        "            logits, hidden = self.forward(last_pred, hidden)\n",
        "            output = F.softmax(logits, dim=1)\n",
        "            #last_pred = torch.argmax(output).unsqueeze(-1)     \n",
        "            distribution = Categorical(logits=output)\n",
        "            last_pred = distribution.sample()\n",
        "            predictions.append(last_pred)\n",
        "        return predictions\n",
        "        \n",
        "\n",
        "vocab_size = len(ctoi_)\n",
        "embedding_dim = 5\n",
        "hidden_dim = 20\n",
        "\n",
        "print(input_seq, target_seq)\n",
        "charRNN = CharRNN(vocab_size, embedding_dim, hidden_dim)\n",
        "train_overfit(charRNN, input_seq, target_seq)\n",
        "\n",
        "print(\"hello\" + tensor_to_text(charRNN.predict_argmax(text_to_tensor(overfit_data,ctoi_),7),itoc_))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9912f5cc-3627-41e8-b5ef-6d30f7c4868a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "9912f5cc-3627-41e8-b5ef-6d30f7c4868a",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "e56bfe8d33a343e270cfe35720aeea26",
          "grade": false,
          "grade_id": "cell-6389d46b2b8abaa0",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 3. Train the RNN model on text data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8024df30-af42-4ca9-be0e-16614ead56cb",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8024df30-af42-4ca9-be0e-16614ead56cb",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "44f87a393c4ae266b59141953d170a7e",
          "grade": false,
          "grade_id": "rnn-train",
          "locked": true,
          "points": 4,
          "schema_version": 3,
          "solution": false,
          "task": true
        }
      },
      "source": [
        "**(Question)** Adapt your previous code to implement a proper training loop for a text dataset. To do so, we need to specify a sequence length `seq_len`, acting similarly to the batch size in classic neural networks. Then, you can either randomly sample sequences of length `seq_len` from the text dataset over `n_iters` iterations, or properly loop over the text dataset for `n_epochs` epochs (with a random starting point for each epoch to ensure different sequences), to make sure the whole dataset is seen by the model. Feel free to adjust training and model parameters empirically. Start with a small model and a small subset of the text dataset, then move on to larger experiments. Remember to use GPU if available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 185,
      "id": "f65d237e-d095-45c4-ab12-6307a5bda255",
      "metadata": {
        "deletable": false,
        "id": "f65d237e-d095-45c4-ab12-6307a5bda255",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "17faecb7751e77b7fe8cae66820687ea",
          "grade": true,
          "grade_id": "cell-a4695fabcf78e1a8",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([10, 16, 56, 30, 58, 10, 16, 57, 20, 56])\n",
            "Epoch 1, Loss: 6.495410442352295\n",
            "Epoch 2, Loss: 5.965396404266357\n",
            "Epoch 3, Loss: 5.257569789886475\n",
            "Epoch 4, Loss: 4.40095853805542\n",
            "Epoch 5, Loss: 3.839010715484619\n",
            "Epoch 6, Loss: 3.49481463432312\n",
            "Epoch 7, Loss: 3.144225835800171\n",
            "Epoch 8, Loss: 2.70739483833313\n",
            "Epoch 9, Loss: 2.3990097045898438\n",
            "Epoch 10, Loss: 1.7991336584091187\n",
            "Epoch 11, Loss: 1.300616979598999\n",
            "Epoch 12, Loss: 1.2830010652542114\n",
            "Epoch 13, Loss: 0.8495723009109497\n",
            "Epoch 14, Loss: 0.7639928460121155\n",
            "Epoch 15, Loss: 0.6662375330924988\n",
            "Epoch 16, Loss: 0.586346447467804\n",
            "Epoch 17, Loss: 0.5872800946235657\n",
            "Epoch 18, Loss: 0.3733738660812378\n",
            "Epoch 19, Loss: 0.4009561240673065\n",
            "Epoch 20, Loss: 0.29682981967926025\n",
            "Epoch 21, Loss: 0.29772889614105225\n",
            "Epoch 22, Loss: 0.25221002101898193\n",
            "Epoch 23, Loss: 0.3406127393245697\n",
            "Epoch 24, Loss: 0.2938058376312256\n",
            "Epoch 25, Loss: 0.3220052421092987\n",
            "Epoch 26, Loss: 0.22417937219142914\n",
            "Epoch 27, Loss: 0.27561044692993164\n",
            "Epoch 28, Loss: 0.2040981501340866\n",
            "Epoch 29, Loss: 0.18240216374397278\n",
            "Epoch 30, Loss: 0.17194764316082\n",
            "Epoch 31, Loss: 0.21624743938446045\n",
            "Epoch 32, Loss: 0.21059875190258026\n",
            "Epoch 33, Loss: 0.2009187936782837\n",
            "Epoch 34, Loss: 0.1773662120103836\n",
            "Epoch 35, Loss: 0.1574215292930603\n",
            "Epoch 36, Loss: 0.15274415910243988\n",
            "Epoch 37, Loss: 0.14642195403575897\n",
            "Epoch 38, Loss: 0.1297912448644638\n",
            "Epoch 39, Loss: 0.11613135039806366\n",
            "Epoch 40, Loss: 0.11587325483560562\n",
            "Epoch 41, Loss: 0.11960937082767487\n",
            "Epoch 42, Loss: 0.15338562428951263\n",
            "Epoch 43, Loss: 0.14349263906478882\n",
            "Epoch 44, Loss: 0.11169736832380295\n",
            "Epoch 45, Loss: 0.11997991800308228\n",
            "Epoch 46, Loss: 0.0995817556977272\n",
            "Epoch 47, Loss: 0.09754904359579086\n",
            "Epoch 48, Loss: 0.08434280008077621\n",
            "Epoch 49, Loss: 0.0873713344335556\n",
            "Epoch 50, Loss: 0.08041716367006302\n",
            "Epoch 51, Loss: 0.08512093126773834\n",
            "Epoch 52, Loss: 0.07647369801998138\n",
            "Epoch 53, Loss: 0.0920397937297821\n",
            "Epoch 54, Loss: 0.08401352167129517\n",
            "Epoch 55, Loss: 0.07367086410522461\n",
            "Epoch 56, Loss: 0.07283727824687958\n",
            "Epoch 57, Loss: 0.06957261264324188\n",
            "Epoch 58, Loss: 0.07024439424276352\n",
            "Epoch 59, Loss: 0.07106383144855499\n",
            "Epoch 60, Loss: 0.07170131057500839\n",
            "Epoch 61, Loss: 0.06857439875602722\n",
            "Epoch 62, Loss: 0.06627750396728516\n",
            "Epoch 63, Loss: 0.0735207125544548\n",
            "Epoch 64, Loss: 0.07298215478658676\n",
            "Epoch 65, Loss: 0.06377414613962173\n",
            "Epoch 66, Loss: 0.08313886821269989\n",
            "Epoch 67, Loss: 0.05628898739814758\n",
            "Epoch 68, Loss: 0.05971601605415344\n",
            "Epoch 69, Loss: 0.05909665301442146\n",
            "Epoch 70, Loss: 0.08800198137760162\n",
            "Epoch 71, Loss: 0.08622759580612183\n",
            "Epoch 72, Loss: 0.053801342844963074\n",
            "Epoch 73, Loss: 0.08773333579301834\n",
            "Epoch 74, Loss: 0.0913449078798294\n",
            "Epoch 75, Loss: 0.0809936672449112\n",
            "Epoch 76, Loss: 0.05733689293265343\n",
            "Epoch 77, Loss: 0.06175610423088074\n",
            "Epoch 78, Loss: 0.060916345566511154\n",
            "Epoch 79, Loss: 0.053963713347911835\n",
            "Epoch 80, Loss: 0.04130886122584343\n",
            "Epoch 81, Loss: 0.07090030610561371\n",
            "Epoch 82, Loss: 0.0559614896774292\n",
            "Epoch 83, Loss: 0.05821896716952324\n",
            "Epoch 84, Loss: 0.04195196181535721\n",
            "Epoch 85, Loss: 0.06611994653940201\n",
            "Epoch 86, Loss: 0.028218969702720642\n",
            "Epoch 87, Loss: 0.09060309827327728\n",
            "Epoch 88, Loss: 0.058269865810871124\n",
            "Epoch 89, Loss: 0.0940685123205185\n",
            "Epoch 90, Loss: 0.091214120388031\n",
            "Epoch 91, Loss: 0.051159173250198364\n",
            "Epoch 92, Loss: 0.055316850543022156\n",
            "Epoch 93, Loss: 0.06825900077819824\n",
            "Epoch 94, Loss: 0.12069525569677353\n",
            "Epoch 95, Loss: 0.0786333978176117\n",
            "Epoch 96, Loss: 0.0728301852941513\n",
            "Epoch 97, Loss: 0.08567051589488983\n",
            "Epoch 98, Loss: 0.0385802686214447\n",
            "Epoch 99, Loss: 0.07475341111421585\n",
            "Epoch 100, Loss: 0.05793764069676399\n"
          ]
        }
      ],
      "source": [
        "# Create the text dataset, compute its mappings and convert it to tensor\n",
        "# YOUR CODE HERE\n",
        "seq_len = 10\n",
        "dataset_size = int(len(text_data) / seq_len) + 1\n",
        "text_dataset_input = torch.empty((dataset_size, seq_len)).long()\n",
        "text_dataset_output = torch.empty((dataset_size, seq_len)).long()\n",
        "for i in range(1,len(text_data), seq_len):\n",
        "    target_input = text_to_tensor(text_data[i-1:i-1+seq_len], ctoi)\n",
        "    target_output = text_to_tensor(text_data[i:i + seq_len], ctoi)\n",
        "    if target_input.size(dim=0) != seq_len:\n",
        "        pad = torch.cat([torch.tensor([len(ctoi) - 1]) for _ in range(seq_len - target_input.size(dim=0))])\n",
        "        target_input = torch.cat((target_input,pad))\n",
        "    if target_output.size(dim=0) != seq_len:\n",
        "        pad = torch.cat([torch.tensor([len(ctoi) - 1]) for _ in range(seq_len - target_output.size(dim=0))])\n",
        "        target_output = torch.cat((target_output,pad))\n",
        "    text_dataset_input[int(i / seq_len)] = target_input\n",
        "    text_dataset_output[int(i / seq_len)] = target_output\n",
        "#print(text_to_tensor(text_data[i:i+seq_len],ctoi))\n",
        "print(text_dataset_input[0])\n",
        "\n",
        "# Initialize training parameters\n",
        "# YOUR CODE HERE\n",
        "vocab_size = len(ctoi)\n",
        "embedding_dim = 32\n",
        "hidden_dim = 256\n",
        "n_epochs = 100\n",
        "# Initialize a character-level RNN model\n",
        "# YOUR CODE HERE\n",
        "textRNN = CharRNN(vocab_size, embedding_dim, hidden_dim)\n",
        "\n",
        "optimizer = torch.optim.SGD(textRNN.parameters(), lr = 0.01)#, weight_decay=5e-3, momentum=0.9)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Setup the training loop\n",
        "# Regularly record the loss and sample from the model to monitor what is happening\n",
        "# YOUR CODE HERE\n",
        "def fit(model, text_dataset_input, text_dataset_output, dataset_size,n_epochs, optimizer):\n",
        "  hidden = None\n",
        "  for iter in range(1, n_epochs + 1):\n",
        "    start_idx = torch.randint(0,int(seq_len / 2),(1,)).item() #randomly select a start_idx\n",
        "    for i in range(dataset_size): \n",
        "      logits, hidden = model(text_dataset_input[i][start_idx:],hidden)\n",
        "      hidden = hidden.detach() #Once we update the hidden state we need to detach it, to not backpropagate through it in the next batch\n",
        "      loss = criterion(logits, text_dataset_output[i][start_idx:])\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      \n",
        "    print(f'Epoch {iter}, Loss: {loss.item()}')\n",
        "  return model\n",
        "\n",
        "textRNN = fit(textRNN, text_dataset_input, text_dataset_output, dataset_size, n_epochs, optimizer)\n",
        "      "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a318abd-5c3c-462d-9944-9aec1f78446c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7a318abd-5c3c-462d-9944-9aec1f78446c",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "113032562a0b82e504d636abf3164360",
          "grade": false,
          "grade_id": "rnn-predict",
          "locked": true,
          "points": 2,
          "schema_version": 3,
          "solution": false,
          "task": true
        }
      },
      "source": [
        "**(Question)** From your trained model, play around with its predictions: start with a custom input sequence and ask the model to predict the rest. Analyze and comment your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 186,
      "id": "53d516b1-477c-4237-956a-471062dd6019",
      "metadata": {
        "deletable": false,
        "id": "53d516b1-477c-4237-956a-471062dd6019",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "5bd724d3a1f3ba5d0b965b9cf7905160",
          "grade": true,
          "grade_id": "cell-08bfe03b817a9908",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " que toursasier la palerinations de la pares\n",
            "Qui palais,\n",
            "Ces palais,\n",
            "Ces palais,\n",
            "Ces palais,\n",
            "Ces palais,\n",
            "Ces palais,\n",
            "Ces palais,\n",
            "Ces palais,\n",
            "Ces palais,\n",
            "Ces palais,\n",
            "Ces palais,\n",
            "Ces palais,\n",
            "Ces palais,\n",
            "\n"
          ]
        }
      ],
      "source": [
        "context_tensor = text_to_tensor(text_data[100:150],ctoi)\n",
        "print(tensor_to_text(textRNN.predict_argmax(context_tensor,200),itoc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad02a4a5-5aab-403a-9bd3-fd752dc1dc9a",
      "metadata": {
        "deletable": false,
        "id": "ad02a4a5-5aab-403a-9bd3-fd752dc1dc9a",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "288f4746f3c7d1906ca99b35f4a6a6e3",
          "grade": true,
          "grade_id": "cell-6b41d47e15ea128a",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7de32122-8819-4d6b-8f7a-e96f671311d6",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7de32122-8819-4d6b-8f7a-e96f671311d6",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "2eddbf9984d6a4d51a7ea1301800bdf3",
          "grade": false,
          "grade_id": "cell-a69a65798f792cfc",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        }
      },
      "source": [
        "## 4. Experiment with different RNN architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5def498-9119-45fd-8807-8260cd0a05d8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "d5def498-9119-45fd-8807-8260cd0a05d8",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "977d70928beb993f5bc6323199b1e363",
          "grade": false,
          "grade_id": "rnn-experiments",
          "locked": true,
          "points": 3,
          "schema_version": 3,
          "solution": false,
          "task": true
        }
      },
      "source": [
        "**(Question)** Experiment with different RNN architecures. Potential ideas are multi-layer RNNs, GRUs and LSTMs. All models can be extended to multi-layer using the `num_layers` parameter. Analyze and comment your results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "id": "38c6fe7d-d159-401d-9c7c-f7e9a5b86cba",
      "metadata": {
        "deletable": false,
        "id": "38c6fe7d-d159-401d-9c7c-f7e9a5b86cba",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "77ebb731623feb1292e7758788ad56d4",
          "grade": true,
          "grade_id": "cell-7bbcfb8355f44b5d",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "class GruNN(CharRNN):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
        "        '''Initialize model parameters and layers.'''\n",
        "        super().__init__(vocab_size, embedding_dim, hidden_dim, num_layers=1)\n",
        "        # YOUR CODE HERE\n",
        "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tensor_data, hidden_state=None):\n",
        "        '''Apply the forward pass for some text data already converted to tensor.'''\n",
        "        # YOUR CODE HERE\n",
        "        embedding = self.embedding(tensor_data)\n",
        "        output, hidden = self.gru(embedding, hidden_state)\n",
        "        logits = self.dense(output)\n",
        "        return logits, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "class LSTMNN(CharRNN):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1):\n",
        "        '''Initialize model parameters and layers.'''\n",
        "        super().__init__(vocab_size, embedding_dim, hidden_dim)\n",
        "        # YOUR CODE HERE\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, tensor_data, hidden_state=None, c=None):\n",
        "        '''Apply the forward pass for some text data already converted to tensor.'''\n",
        "        # YOUR CODE HERE\n",
        "        embedding = self.embedding(tensor_data)\n",
        "        output, (hidden, c) = self.lstm(embedding, hidden_state, c)\n",
        "        logits = self.dense(output)\n",
        "        return logits, hidden, c\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = len(ctoi)\n",
        "embedding_dim = 32\n",
        "hidden_dim = 256\n",
        "\n",
        "n_epochs = 50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {},
      "outputs": [],
      "source": [
        "context_tensor = text_to_tensor(text_data[10:50], ctoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 5.897712707519531\n",
            "Epoch 2, Loss: 4.4717888832092285\n",
            "Epoch 3, Loss: 4.11949348449707\n",
            "Epoch 4, Loss: 4.1267828941345215\n",
            "Epoch 5, Loss: 3.3360939025878906\n",
            "Epoch 6, Loss: 2.8215410709381104\n",
            "Epoch 7, Loss: 3.0039632320404053\n",
            "Epoch 8, Loss: 1.9431304931640625\n",
            "Epoch 9, Loss: 1.7728513479232788\n",
            "Epoch 10, Loss: 1.508933663368225\n",
            "Epoch 11, Loss: 1.7067683935165405\n",
            "Epoch 12, Loss: 0.9359719753265381\n",
            "Epoch 13, Loss: 0.8309974670410156\n",
            "Epoch 14, Loss: 1.137855052947998\n",
            "Epoch 15, Loss: 0.6840835213661194\n",
            "Epoch 16, Loss: 0.6545213460922241\n",
            "Epoch 17, Loss: 0.5482220649719238\n",
            "Epoch 18, Loss: 0.5534723401069641\n",
            "Epoch 19, Loss: 0.5304295420646667\n",
            "Epoch 20, Loss: 0.4010707437992096\n",
            "Epoch 21, Loss: 0.5849392414093018\n",
            "Epoch 22, Loss: 0.31918802857398987\n",
            "Epoch 23, Loss: 0.4908319413661957\n",
            "Epoch 24, Loss: 0.3148772716522217\n",
            "Epoch 25, Loss: 0.3190081715583801\n",
            "Epoch 26, Loss: 0.32696613669395447\n",
            "Epoch 27, Loss: 0.3534165918827057\n",
            "Epoch 28, Loss: 0.21998031437397003\n",
            "Epoch 29, Loss: 0.27933716773986816\n",
            "Epoch 30, Loss: 0.2111043483018875\n",
            "Epoch 31, Loss: 0.21350476145744324\n",
            "Epoch 32, Loss: 0.21638646721839905\n",
            "Epoch 33, Loss: 0.2754330039024353\n",
            "Epoch 34, Loss: 0.26739782094955444\n",
            "Epoch 35, Loss: 0.2852495014667511\n",
            "Epoch 36, Loss: 0.20523089170455933\n",
            "Epoch 37, Loss: 0.14921081066131592\n",
            "Epoch 38, Loss: 0.26774561405181885\n",
            "Epoch 39, Loss: 0.24197232723236084\n",
            "Epoch 40, Loss: 0.1352825164794922\n",
            "Epoch 41, Loss: 0.18486309051513672\n",
            "Epoch 42, Loss: 0.23197226226329803\n",
            "Epoch 43, Loss: 0.15926384925842285\n",
            "Epoch 44, Loss: 0.11800378561019897\n",
            "Epoch 45, Loss: 0.20396018028259277\n",
            "Epoch 46, Loss: 0.1551562398672104\n",
            "Epoch 47, Loss: 0.13156040012836456\n",
            "Epoch 48, Loss: 0.17458783090114594\n",
            "Epoch 49, Loss: 0.11983529478311539\n",
            "Epoch 50, Loss: 0.12912313640117645\n"
          ]
        }
      ],
      "source": [
        "multilayer_rnn = CharRNN(vocab_size, embedding_dim, hidden_dim, num_layers=4)\n",
        "optimizer = torch.optim.SGD(multilayer_rnn.parameters(), lr = 0.01)#, weight_decay=5e-3, momentum=0.9)\n",
        "\n",
        "multilayer_rnn = fit(multilayer_rnn, text_dataset_input, text_dataset_output, dataset_size, n_epochs, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CEnnale violets?\n",
            "\n",
            "Range\n",
            "Rembragé par un boit et la palais,\n",
            "Et de chont leurs par de vine lieu des puissants nous puissants nous puissants nous puissants nous puissants nous puissants nous puissants nou\n"
          ]
        }
      ],
      "source": [
        "print(tensor_to_text(multilayer_rnn.predict_argmax(context_tensor,200),itoc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 4.27915096282959\n",
            "Epoch 2, Loss: 4.315567970275879\n",
            "Epoch 3, Loss: 4.445417881011963\n",
            "Epoch 4, Loss: 4.556044101715088\n",
            "Epoch 5, Loss: 4.79245138168335\n",
            "Epoch 6, Loss: 4.742119789123535\n",
            "Epoch 7, Loss: 4.783855438232422\n",
            "Epoch 8, Loss: 4.859288215637207\n",
            "Epoch 9, Loss: 5.052785873413086\n",
            "Epoch 10, Loss: 4.726766109466553\n",
            "Epoch 11, Loss: 4.696249485015869\n",
            "Epoch 12, Loss: 5.026475429534912\n",
            "Epoch 13, Loss: 4.8592209815979\n",
            "Epoch 14, Loss: 4.640098571777344\n",
            "Epoch 15, Loss: 4.775290489196777\n",
            "Epoch 16, Loss: 4.472657203674316\n",
            "Epoch 17, Loss: 4.677436351776123\n",
            "Epoch 18, Loss: 4.522556304931641\n",
            "Epoch 19, Loss: 4.389292240142822\n",
            "Epoch 20, Loss: 4.524266719818115\n",
            "Epoch 21, Loss: 4.369021892547607\n",
            "Epoch 22, Loss: 4.424742698669434\n",
            "Epoch 23, Loss: 4.375044345855713\n",
            "Epoch 24, Loss: 4.145031452178955\n",
            "Epoch 25, Loss: 4.1023783683776855\n",
            "Epoch 26, Loss: 3.9952476024627686\n",
            "Epoch 27, Loss: 4.200692176818848\n",
            "Epoch 28, Loss: 4.059335708618164\n",
            "Epoch 29, Loss: 3.9473016262054443\n",
            "Epoch 30, Loss: 3.9865736961364746\n",
            "Epoch 31, Loss: 4.227707386016846\n",
            "Epoch 32, Loss: 4.185577869415283\n",
            "Epoch 33, Loss: 4.146249771118164\n",
            "Epoch 34, Loss: 3.8382434844970703\n",
            "Epoch 35, Loss: 3.7413995265960693\n",
            "Epoch 36, Loss: 4.05653715133667\n",
            "Epoch 37, Loss: 3.6852972507476807\n",
            "Epoch 38, Loss: 3.6042397022247314\n",
            "Epoch 39, Loss: 3.5873875617980957\n",
            "Epoch 40, Loss: 3.6315972805023193\n",
            "Epoch 41, Loss: 3.9454801082611084\n",
            "Epoch 42, Loss: 3.5786867141723633\n",
            "Epoch 43, Loss: 3.6228952407836914\n",
            "Epoch 44, Loss: 3.689061403274536\n",
            "Epoch 45, Loss: 3.5676791667938232\n",
            "Epoch 46, Loss: 3.4183430671691895\n",
            "Epoch 47, Loss: 3.7999789714813232\n",
            "Epoch 48, Loss: 3.497671365737915\n",
            "Epoch 49, Loss: 3.5670478343963623\n",
            "Epoch 50, Loss: 3.5409789085388184\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "multilayer_gru = GruNN(vocab_size, embedding_dim, hidden_dim, num_layers=2)\n",
        "optimizer = torch.optim.SGD(multilayer_gru.parameters(), lr = 0.001)#, weight_decay=5e-3, momentum=0.9)\n",
        "multilayer_gru = fit(multilayer_gru, text_dataset_input, text_dataset_output, dataset_size, n_epochs, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " an de les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les les le\n"
          ]
        }
      ],
      "source": [
        "print(tensor_to_text(multilayer_gru.predict_argmax(context_tensor,200),itoc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_lstm(model, text_dataset_input, text_dataset_output, dataset_size,n_epochs, optimizer):\n",
        "  hidden = None\n",
        "  c = None\n",
        "  for iter in range(1, n_epochs + 1):\n",
        "    start_idx = torch.randint(0,int(seq_len / 2),(1,)).item() #randomly select a start_idx\n",
        "    for i in range(dataset_size): \n",
        "      logits, hidden, c = model(text_dataset_input[i][start_idx:], hidden, c)\n",
        "      print(hidden.size())\n",
        "      hidden = hidden.detach()\n",
        "      c = c.detach() #Once we update the hidden state we need to detach it, to not backpropagate through it in the next batch\n",
        "      loss = criterion(logits, text_dataset_output[i][start_idx:])\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "        \n",
        "    print(f'Epoch {iter}, Loss: {loss.item()}')\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "LSTM.forward() takes from 2 to 3 positional arguments but 4 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ayman\\IS319-Deep-Learning\\tp3\\dl_tp3.ipynb Cell 43\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X60sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m multilayer_lstm \u001b[39m=\u001b[39m LSTMNN(vocab_size, embedding_dim, hidden_dim, num_layers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X60sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(multilayer_lstm\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m)\u001b[39m#, weight_decay=5e-3, momentum=0.9)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X60sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m multilayer_lstm \u001b[39m=\u001b[39m fit_lstm(multilayer_lstm, text_dataset_input, text_dataset_output, dataset_size, n_epochs, optimizer)\n",
            "\u001b[1;32mc:\\Users\\ayman\\IS319-Deep-Learning\\tp3\\dl_tp3.ipynb Cell 43\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X60sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39miter\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, n_epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X60sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m   \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(dataset_size): \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X60sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     logits, hidden, c \u001b[39m=\u001b[39m model(text_dataset_input[i][start_idx:], hidden, c)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X60sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mprint\u001b[39m(hidden\u001b[39m.\u001b[39msize())\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X60sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     hidden \u001b[39m=\u001b[39m hidden\u001b[39m.\u001b[39mdetach()\n",
            "File \u001b[1;32mc:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;32mc:\\Users\\ayman\\IS319-Deep-Learning\\tp3\\dl_tp3.ipynb Cell 43\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X60sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X60sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(tensor_data)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X60sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m output, (hidden, c) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(embedding, hidden_state, c)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X60sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X60sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m logits, hidden, c\n",
            "File \u001b[1;32mc:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;31mTypeError\u001b[0m: LSTM.forward() takes from 2 to 3 positional arguments but 4 were given"
          ]
        }
      ],
      "source": [
        "multilayer_lstm = LSTMNN(vocab_size, embedding_dim, hidden_dim, num_layers=4)\n",
        "optimizer = torch.optim.SGD(multilayer_lstm.parameters(), lr = 0.01)#, weight_decay=5e-3, momentum=0.9)\n",
        "\n",
        "multilayer_lstm = fit_lstm(multilayer_lstm, text_dataset_input, text_dataset_output, dataset_size, n_epochs, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "LSTM.forward() takes from 2 to 3 positional arguments but 4 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\ayman\\IS319-Deep-Learning\\tp3\\dl_tp3.ipynb Cell 44\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X61sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(tensor_to_text(multilayer_lstm\u001b[39m.\u001b[39;49mpredict_argmax(context_tensor,\u001b[39m200\u001b[39;49m),itoc))\n",
            "\u001b[1;32mc:\\Users\\ayman\\IS319-Deep-Learning\\tp3\\dl_tp3.ipynb Cell 44\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X61sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict_argmax\u001b[39m(\u001b[39mself\u001b[39m, context_tensor, n_predictions):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X61sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m# Apply the forward pass for the context tensor\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X61sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# Then, store the last prediction and last hidden state\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X61sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X61sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     predictions \u001b[39m=\u001b[39m []\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X61sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     logits, hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(context_tensor)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X61sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     output \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X61sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     last_pred \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(output)\n",
            "\u001b[1;32mc:\\Users\\ayman\\IS319-Deep-Learning\\tp3\\dl_tp3.ipynb Cell 44\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X61sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X61sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(tensor_data)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X61sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m output, (hidden, c) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlstm(embedding, hidden_state, c)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X61sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdense(output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/ayman/IS319-Deep-Learning/tp3/dl_tp3.ipynb#X61sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m logits, hidden, c\n",
            "File \u001b[1;32mc:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\ayman\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
            "\u001b[1;31mTypeError\u001b[0m: LSTM.forward() takes from 2 to 3 positional arguments but 4 were given"
          ]
        }
      ],
      "source": [
        "print(tensor_to_text(multilayer_lstm.predict_argmax(context_tensor,200),itoc))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13329eaa-6a48-47cf-912d-424a79680f91",
      "metadata": {
        "deletable": false,
        "id": "13329eaa-6a48-47cf-912d-424a79680f91",
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "5bffac2e6bdfed739aea204b3c40a792",
          "grade": true,
          "grade_id": "cell-3961b7f97f038a4b",
          "locked": false,
          "points": 0,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "source": [
        "YOUR ANSWER HERE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
